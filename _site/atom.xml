<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>1SoulJo</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2018-02-19T10:43:01+00:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>1SoulJo</name>
   <email></email>
 </author>

 
 <entry>
   <title>Entropy, Cross-entropy and KL-divergence</title>
   <link href="http://localhost:4000/study/2018/02/19/entropy/"/>
   <updated>2018-02-19T15:33:00+00:00</updated>
   <id>http://localhost:4000/study/2018/02/19/entropy</id>
   <content type="html">&lt;p&gt;영상 : &lt;a href=&quot;https://www.youtube.com/watch?v=ErfnhcEV1O8&amp;amp;t=186s&quot;&gt;A Short Introduction to Entropy, Cross-Entropy and KL-Divergence&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;위 링크 영상에서 Entropy, Cross-entropy, KL-divergence 에 대해 이해하기 쉽게 설명을 해주고 있어, 정리 차원에서 작성해봅니다.&lt;/p&gt;

&lt;h2 id=&quot;불확실성uncertainty-과-bit&quot;&gt;불확실성(Uncertainty) 과 bit&lt;/h2&gt;

&lt;p&gt;Entropy, Cross-entropy 의 개념은 미국의 수학자이자 암호학자인 Claude Shannon 의 1948년 논문 “A Mathematical Theory of Communication” 으로부터 시작되었습니다. 그는 논문에서 현재의 정보 이론(Information Theory)으로 불리는 개념을 처음 소개하였습니다.
디지털 통신에서 메세지는 bit 들로 이루어져 있는데, 알다시피 0 또는 1로 구성됩니다. 하지만 모든 bit 가 유용한 정보를 포함하지는 않습니다.&lt;/p&gt;

&lt;p&gt;Shannon 은 자신의 이론에서 이렇게 설명합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1 bit 의 정보를 전송하는 것은, 수신자의 불확실성을 2로 나누는 것을 의미한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;직관적으로 잘 이해가되지는 않지만, 일기예보의 예를 들어보겠습니다. 내일 비가 올 확률이 50%, 맑을 확률이 50% 라고 합시다.
만약 기상청에서 내일 비가 온다고 예보를 했다면, 이는 우리의 불확실성을 2로 나눈 셈입니다.
같은 확률의 2가지 가능성이 있었는데, 예보로 인해 이것이 1로 줄었기 때문이죠.
따라서 기상청은 실제로는 1 bit 의 유용한 정보를 보낸 것이라고 할 수 있습니다. 이것은 실제로 이 정보가 어떻게 인코딩되었는지와는 무관합니다.
“Rainy” 라는 문자열로 전달되었다면 데이터의 크기는 40 bit 이겠지만 실제 유용한 정보는 1 bit 인 것이죠.&lt;/p&gt;

&lt;p&gt;그럼 내일의 날씨를 8 가지로 구분할 수 있다면 어떨까요?&lt;br /&gt;
예를 들면 ‘맑음, 약간 흐림, 흐림, 많이 흐림, 약간 비, 비, 많은 비, 천둥번개’ 라고 할 수도 있겠죠.
따라서 기상청이 내일의 일기 예보를 우리에게 보낸다면, 이는 우리의 불확실성을 8로 나눈 것이라고 할 수 있습니다.
8은 2의 3승이니까, 이 일기 예보는 3 bit 의 유용한 정보를 보낸 셈이죠.&lt;/p&gt;

&lt;p&gt;이렇게 보면 ‘실제로 전달된 정보’의 bit 수를 계산하는건 간단해 보입니다. 불확실성을 줄여주는 인수(factor) 값에 로그2 를 씌워주면 되겠죠.&lt;br /&gt;
위 예제라면, 아래 식으로 설명될겁니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log_2(8)=3&lt;/script&gt;

&lt;p&gt;지금까지 설명한 것은 각각의 날씨들이 모두 같은 확률로 발생할 수 있다고 가정했는데요,&lt;br /&gt;
각각의 확률이 다르다면 어떻게 될까요?
예를 들어, 내일 비가 올 확률이 25%, 맑을 확률은 75% 라고 해봅시다.&lt;br /&gt;
이 때 기상청에서 내일 비가 올 것이라고 예보했다면, 이로 인해 우리의 불확실성은 1/2 이 아니라 1/4 로 줄었다고 할 수 있습니다.&lt;br /&gt;
불확실성이라는 개념이 조금 헷갈리지만, 50%, 50% 의 가능성을 가진 결과에서 하나를 예측한 것보다는 25%, 75% 의 가능성의 가진 결과 중에 25% 일 것이라고 예측하는 쪽이 더 많은 불확실성을 낮춰줬다고 말하는게 당연하겠죠.&lt;br /&gt;
만약에 비가 거의 오지 않는 사막에서 (비 올 확률 1%), 내일의 일기 예보가 ‘비’ 라고 한다면, 이는 불확실성이 1/100 로 훨씬 많이 줄어들었다고 할 수 있을 것입니다. 즉, 같은 ‘비’ 예보라고 하더라도, 비 올 확률이 50% 인 장소에서의 의미와, 1% 인 장소에서의 의미는 다를 것입니다. 당연히 1% 인 곳에서는 더 많은 불확실성이 해소된거죠.&lt;/p&gt;

&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;

&lt;p&gt;위 설명을 공식화해서 설명해보면,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;불확실성이 감소되는 크기는 어떤 이벤트의 발생 확률의 역수와 같다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;라고 할 수 있습니다. 위 예제의 경우, 25% 의 역수이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;1 / 0.25 = 4&lt;/script&gt;

&lt;p&gt;이고, 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log_2(4) = 2&lt;/script&gt;

&lt;p&gt;이니까 2 bit 의 정보가 필요한게 됩니다. 이 때 역수의 log 는 - 를 붙이는 것과 같으니까,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log_2(4) = -log_2(0.25)&lt;/script&gt;

&lt;p&gt;로 정리할 수 있겠네요.&lt;/p&gt;

&lt;p&gt;만약 위 예제에서 기상청이 내일 날씨가 맑을거라고 예보하는 경우, 불확실성은 그렇게 많이 낮아지지 않을겁니다.&lt;/p&gt;

&lt;p&gt;$ -log_2(0.75) = 0.41 $&lt;/p&gt;

&lt;p&gt;즉, 0.41 bit 가 됩니다. 0.41 bit 만큼의 정보를 얻을 수 있다는 뜻이죠.&lt;/p&gt;

&lt;p&gt;그럼 기상청으로부터 얻을 수 있는 정보의 평균값은 얼마일까요? 각각의 이벤트에 대한 기대값을 계산해보면,&lt;/p&gt;

&lt;p&gt;$ 75\% * 0.41 + 25\% * 2 = 0.81 \text{ bits} $&lt;/p&gt;

&lt;p&gt;즉, 평균적으로 우리는 매일 기상청으로부터 0.81 bit 의 정보를 얻을 수 있다고 할 수 있습니다.&lt;br /&gt;
이것이 바로 &lt;strong&gt;Entropy&lt;/strong&gt; 입니다. Entropy 는 &lt;em&gt;어떤 이벤트가 얼마나 불확실한지&lt;/em&gt; 를 설명하는 아주 훌륭한 방법입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Entropy:} \\
H(p) = -\sum_ip_ilog_2(p_i)&lt;/script&gt;

&lt;p&gt;위 공식이 완전히 이해가 되죠? 요약하자면,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Entropy 는 주어진 확률 분포 p 에서 하나의 샘플로부터 얻을 수 있는 정보의 평균량이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;라고 할 수 있을 것입니다. 이는 해당 확률 분포가 얼마나 예측하기 힘든지를 의미합니다.&lt;br /&gt;
만약 우리가 거의 항상 날씨가 맑은 사막 한가운데에 살고 있다면, 일기 예보로부터 얻을 수 있는 정보는 많지 않습니다.&lt;br /&gt;
Entropy 는 거의 0에 가깝겠죠?&lt;br /&gt;
반대로 날씨가 변화무쌍한 곳이라면 Entropy 값은 훨씬 클 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;cross-entropy-and-kl-divergence&quot;&gt;Cross-entropy and KL Divergence&lt;/h2&gt;

&lt;p&gt;그럼 Cross-entropy 는 뭔지 얘기해볼께요. 사실 아주 간단한 개념입니다.&lt;br /&gt;
바로 &lt;strong&gt;메세지 길이의 평균값&lt;/strong&gt; 입니다. 위에서 설명했던 8가지 날씨 예제에서, 기상청이 아래와 같은 3 bit 의 정보를 사용한다고 해보죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/8_weather_3bits.jpg&quot; alt=&quot;Weathers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 경우 메세지의 평균 길이는 당연히 3 bit 입니다. 그게 Cross-entropy 입니다.&lt;br /&gt;
하지만 이것은 각각의 확률이 같다고 가정한 경우이고, 만약 아래 그림처럼 각각 날씨에 대한 확률이 다른 경우는 어떨까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/8_weather_3bits_2.jpg&quot; alt=&quot;Weathers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 경우 Entropy 를 계산해보면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Entropy} \\
= 0.35log_2(0.35) + \text{ ... } + 0.01log_2(0.01) \\
= 2.23 \text{ bits}&lt;/script&gt;

&lt;p&gt;즉, 실제 유용한 정보는 2.23 bit 뿐임을 알 수 있습니다. 그럼 아래처럼 변경하면 어떨까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/8_weather_3bits_3.jpg&quot; alt=&quot;Weathers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 각각의 날씨에 대한 메세지의 길이를 달리 했습니다.&lt;br /&gt;
이렇게 하면 연속된 bit 들에 대해서도 간단하게 해석이 가능해집니다.&lt;br /&gt;
예를 들어 011100 이라는 메세지는 01 + 1100 만으로 분리 가능하기 때문이죠.&lt;br /&gt;
이때 Cross-entropy 를 다시 계산해보면,&lt;/p&gt;

&lt;p&gt;$ 35\% * 2 + 35\% * 2 + 10\% * 3 + \text{…} + 1\% * 5 = 2.42 \text{ bits} $&lt;/p&gt;

&lt;p&gt;이렇게 Cross-entropy 를 개선시켰습니다. 하지만 3 bit 보다는 낫지만 2.23 bit 까지는 낮추진 못했죠.&lt;br /&gt;
아무튼 이번엔 이 방식을 다른 장소에 적용해보면 어떨까요? 아래처럼 항상 비가 오는 곳이라고 해보죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/8_weather_3bits_4.jpg&quot; alt=&quot;Weathers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 때의 Cross-entropy 를 계산해보면, 4.58 bit 라는 값이 나오게 됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;1\% * 2 + 1\% * 2 + 4\% * 3 + \text{...} + 35\% * 5 = 4.58 \text{ bits}&lt;/script&gt;

&lt;p&gt;굉장히 안좋은 결과네요. 거의 entropy 의 2 배에 가깝습니다.&lt;br /&gt;
다시 말하면, 이 경우 평균적으로 4.58 bit 가 전송되지만 실제로는 2.23 bit 만이 유용하다는 의미입니다.&lt;br /&gt;
이는 우리의 코드가 날씨의 분포에 대해서 가정(assumption) 을 내포하고 있기 때문입니다.&lt;br /&gt;
예를 들어, 맑은 날씨를 2 bit 메세지로 전송하겠다는 것은 최소한 4일에 한 번(2 의 2 승)은 날씨가 맑을 것이라는 걸 내포하고 있습니다.&lt;br /&gt;
즉, 우리의 코드는 25% 확률로 날씨가 맑을 것이라는 암묵적인 예측을 하고 있는 셈입니다. 그게 틀린다면 우리의 코드는 최적화될 수 없겠죠.&lt;/p&gt;

&lt;p&gt;따라서 예측된 분포 q 는 실제 분포 p 와 차이가 발생할 수 밖에 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/8_weather_3bits_5.jpg&quot; alt=&quot;Weathers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 Cross-entropy 를 실제 확률 분포 p 와, 예측된 확률 분포 q 의 공식으로 설명이 가능해집니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Cross-Entropy :} \\
H(p, q) = -\sum_i{p_ilog_2(q_i)}&lt;/script&gt;

&lt;p&gt;보시다시피 Entropy 공식과 비슷하지만, 실제 분포 p 의 log 대신 예측된 분포 q 의 log 를 사용하고 있습니다.&lt;br /&gt;
여기서 $ log_2(q) $ 는 메세지의 길이죠.&lt;br /&gt;
만약 우리의 예측이 완벽하다면, 즉 예측된 분포가 실제 분포와 동일하다면, Cross-entropy 는 Entropy 와 같은 값을 가질 것입니다.&lt;br /&gt;
하지만 분포가 차이날수록 Cross-entropy 는 Entropy 보다 더 큰 값이 되겠죠.&lt;/p&gt;

&lt;p&gt;이와 같이 Cross-entropy 와 Entropy 의 차이를 &lt;strong&gt;Relative Entorpy&lt;/strong&gt; 라고 부르고, 더 일반적으로&lt;br /&gt;
&lt;strong&gt;Kullback-Leibler Divergence (KL Divergence)&lt;/strong&gt; 라고 부릅니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Cross-Entropy = Entropy + KL Divergence}&lt;/script&gt;

&lt;p&gt;공식으로 적어보면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{KL Divergence :} \\
D_{KL}(p || q) = H(p, q) - H(p)&lt;/script&gt;

&lt;p&gt;위의 예제에서, Cross-entropy 는 4.58 bit 이고 Entropy 가 2.23 bit 였으니까 KL Divergence 는 2.35 bit 가 되겠네요.&lt;/p&gt;

&lt;h2 id=&quot;cross-entropy-in-machine-learning&quot;&gt;Cross-entropy in Machine Learning&lt;/h2&gt;

</content>
 </entry>
 
 <entry>
   <title>Ensemble model</title>
   <link href="http://localhost:4000/study/2017/12/20/Ensemble-model/"/>
   <updated>2017-12-20T12:07:00+00:00</updated>
   <id>http://localhost:4000/study/2017/12/20/Ensemble-model</id>
   <content type="html">&lt;p&gt;원본 : &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/&quot;&gt;How to build Ensemble Models in machine learning?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;지난 12개월 간 다양한 머신 러닝 해커톤에 참여해왔다. 시합이 끝나면 항상 1등의 솔루션을 확인해보는데, 보다보면 굉장한 insight 를 받기도 해서 차후 시합에 큰 도움이 되곤 했다.&lt;br /&gt;
우승자들의 대부분은 잘 튜닝된 개별 모델의 앙상블(Ensemble) 과 더불어 feature engineering 을 사용하고 있었다.
이 두 방법을 잘 기억하기를 조언한다. 머신 러닝에 있어서 이 것들을 잘 사용하는 것이 아주 중요하기 때문이다.&lt;br /&gt;
나는 대부분의 경우 feature engineering 은 잘 사용해왔지만, 앙상블은 사용하지 않았다. 만약 당신이 초보자라면 앙상블 모델에
최대한 빨리 익숙해지는 것이 좋을 것이다. (어쩌면 무의식 중에 이미 적용중이었을 수도 있다!)&lt;/p&gt;

&lt;p&gt;이 포스트에서 앙상블 모델링의 기초에 대해 설명하고자 한다. 그리고 나서 앙상블의 장단점을 살펴보고, 실습을 제공할 것이다.&lt;/p&gt;

&lt;h2 id=&quot;1-ensemble-이란&quot;&gt;1. Ensemble 이란?&lt;/h2&gt;
&lt;p&gt;일반적으로 앙상블이란 2개 이상의 알고리즘 - base learner 라고 함 - 들을 결합해서 사용하는 기술을 의미한다.
모든 base learner 들의 예측 결과를 조합하여 사용하기 때문에 전체 시스템을 보다 탄탄하게 해준다.
여러명의 주식 거래자들이 한 방에 모여 주가가 오를지 내릴지를 토론하여 결정하는 것과 마찬가지라고 생각하면 되겠다.&lt;/p&gt;

&lt;p&gt;각각의 거래자들이 모두 다른 정보와 이해를 가지고있기 때문에, 문제 상황으로부터 결론에 이르기까지의 공식은 다 다를 것이다.
따라서 주식 시장에 대한 각자의 이해를 바탕으로 다양한 주가 예측이 나오게 된다.&lt;/p&gt;

&lt;p&gt;이제 최종 결론을 내릴 때가 되면 모든 다양한 예측들을 함께 고려해봐야할 것이다.
이로 인해 우리의 최종 결론은 더욱 탄탄(robust)하고, 정확하고, 덜 편중(biased)된 방향으로 결정될 수 있다.
최종 결론은 거래자 개인의 판단에 기반한 결론과 정 반대의 내용이 될 지도 모른다.&lt;/p&gt;

&lt;p&gt;또 다른 예로 구직자의 인터뷰를 들 수 있다. 구직자가 가진 능력에 대한 최종 판단은 인터뷰 진행자들의 모든 피드백에 기반하여 결정된다.
인터뷰 진행자 한 명만으로는 요구 기술과 특성 각각에 대한 테스트를 모두 수행하지는 못할 수 있다.
하지만 여러명의 인터뷰 진행자들의 복합적인 피드백으로 인해 구직자에 대한 더 올바른 평가가 이루어 질 수 있는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;2-types-of-ensembling&quot;&gt;2. Types of ensembling&lt;/h2&gt;
&lt;p&gt;더 세부적으로 진행하기 전에 꼭 알아야하는 기본 컨셉들이 있다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Averaging&lt;/strong&gt; :&lt;br /&gt;
모델들의 예측 결과에 대한 평균을 취하는 것으로, regression 문제나 classification 확률 예측 문제에 모두 사용된다.
&lt;img src=&quot;/public/img/Average.png&quot; alt=&quot;Average&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Majority vote&lt;/strong&gt; :&lt;br /&gt;
classification 문제를 해결할 때, 각각의 모델이 선정한 결과 중 최대 빈도로 선택된 결과를 취하는 방식이다.
&lt;img src=&quot;/public/img/voting.png&quot; alt=&quot;voting&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Weighted average&lt;/strong&gt; :&lt;br /&gt;
Average 값들에 모델 별 가중치를 두는 방식이다.
&lt;img src=&quot;/public/img/Wtaverage1.png&quot; alt=&quot;Wtaverage&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여러 개의 모델들을 조합하는 방법은 셀 수 없이 많을 것이다. 하지만 주로 사용되는 기술들을 소개한다:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Bagging&lt;/strong&gt;&lt;br /&gt;
Bagging 은 &lt;strong&gt;B&lt;/strong&gt;ootstrap &lt;strong&gt;agg&lt;/strong&gt;regation 이라고도 불린다.
Bagging 을 이해하기 위해서는 먼저 bootstraping 을 이해할 필요가 있다. Bootstraping 은 n 개의 행을 가진 원본 dataset 으로부터 n 개의 행을 선택하는 샘플링 기법이다.
핵심은 각각의 행이 선택될 때 동일한 확률로 선택된다는 것이다.&lt;br /&gt;
  Original dataset
  &lt;img src=&quot;/public/img/BAGG1.png&quot; alt=&quot;Wtaverage&quot; /&gt;&lt;br /&gt;
  1st sampling
  &lt;img src=&quot;/public/img/BAGG2.png&quot; alt=&quot;Wtaverage&quot; /&gt;&lt;br /&gt;
  2nd sampling
  &lt;img src=&quot;/public/img/BAGG3.png&quot; alt=&quot;Wtaverage&quot; /&gt;&lt;br /&gt;
  3rd sampling
  &lt;img src=&quot;/public/img/BAGG4.png&quot; alt=&quot;Wtaverage&quot; /&gt;&lt;br /&gt;
  랜덤하게 선택하기 때문에, 위와 같이 중복 선택이 허용된다.&lt;/p&gt;

    &lt;p&gt;동일한 원본 data 로부터 여러개의 bootstrapped sample 들을 추출할 수 있는데, 추출된 sample 로부터 또 다시 bootstraping 을 하면
 tree 구조의 많은 sample set 을 구성할 수 있다. 그러한 sample set 을 기반으로 majority vote 나 averaging 기법을 사용해서
 최종 예측을 얻을 수 있다. Bagging 이란 이렇게 동작하는 것이다.&lt;/p&gt;

    &lt;p&gt;여기서 눈여겨 보아야 할 것은, bagging 을 통해 분산(variance)를 줄일 수 있다는 점이다. Random forest 가 사실 이러한
 컨셉을 사용하는데, Random forest 는 한 발 더 나아가 각각의 bootstrap 된 sample 들의 feature subset 까지 랜덤하게 추출함을 통해
 분산을 줄인다. 이를 통해 학습 데이터를 구분(split) 할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Boosting&lt;/strong&gt;&lt;br /&gt;
Boosting 은 여러개의 알고리즘을 순차적으로 적용하는 기법으로, 첫 번째 알고리즘이 전체 dataset 에 대한 학습을 진행하고,
다음 알고리즘들은 첫 번째 알고리즘이 실패한(남긴, residuals) 데이터에 맞추어 생성되는 방식이다.
따라서 다음 알고리즘들은 이전 모델이 잘 예측하지 못한 데이터에게 더 큰 가중치를 두게 된다.&lt;br /&gt;
  이는 전체 dataset 에 대한 성능은 좋지 않지만, 특정 부분에 대한 성능은 좋은 learner 들의 집합으로 이루어진다.
  따라서 각각의 모델이 모여 앙상블 전체의 성능을 향상(boost) 시키는 것이다.&lt;br /&gt;
  여기서 중요한 부분은, &lt;i&gt;boosting 은 편중(bias) 를 줄이는 데에 집중한다&lt;/i&gt; 는 점이다.
  이로 인해 boosting 은 오버피팅을 유발하기 쉽다. 따라서 boosting 사용 시, 오버피팅을 피하기 위한 파라메터 튜닝이 굉장히 중요하다.&lt;br /&gt;
  Boosting 의 대표적인 예제로는 XGBoost, GBM, ADABOOST 등이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Stacking&lt;/strong&gt;&lt;br /&gt;
Stacking 은 여러개의 머신 러닝 모델들을 쌓아서 각각의 모델이 예측 결과를 상위 레이어의 모델에 전달하고,
최상위의 모델이 최종 결론을 도출하는 방식이다.
&lt;img src=&quot;/public/img/stacking.png&quot; alt=&quot;Stacking&quot; /&gt;
위 그림에는 두 개의 머신 러닝 모델들이 존재하고 있다:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;하위 레이어의 모델들(d1, d2, d3) 는 원본 입력 feature들(x) 를 취한다.&lt;/li&gt;
      &lt;li&gt;최상위 모델 f() 는 하위 레이어 모델의 출력을 받아 최종 출력을 예측한다.&lt;/li&gt;
      &lt;li&gt;주목할 점은 예측된 결과 값들이 학습 data 로 사용된다는 점이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위에서는 2 개의 레이어만 사용되었지만, 다양한 수의 레이어가 사용될 수 있고, 레이어 내의 모델 역시 개수에 제한이 없다.
모델들을 선택함에 있어 두 가지 핵심 요건이 있는데:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;개별 모델들은 정해둔 정확도 요건(criteria)을 충족시킬 것.&lt;/li&gt;
  &lt;li&gt;개별 모델의 예측 값은 다른 모델의 예측과 큰 연관도(correlation)가 없어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;최상위 모델은 하위 모델의 예측 결과를 입력값으로 받고 있는데, 이 최상위 모델 또한 아래와 같은 단순한 모델로 대체될 수 있다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Averaging&lt;/li&gt;
  &lt;li&gt;Majority vote&lt;/li&gt;
  &lt;li&gt;Weighted average&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-ensemble-의-장단점&quot;&gt;3. Ensemble 의 장단점&lt;/h2&gt;
&lt;h4 id=&quot;31-장점&quot;&gt;3.1 장점&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;앙상블은 모델의 정확도를 높이고, 대부분의 case 에 대해 잘 동작한다고 입증된 방식이다.&lt;/li&gt;
  &lt;li&gt;거의 모든 머신 러닝 해커톤에 있어서, 1위를 하는데 필수적이다.&lt;/li&gt;
  &lt;li&gt;앙상블은 모델을 더욱 탄탄하고 안정적이게 해주고, 따라서 대부분의 시나리오에서 괜찮은 성능을 보장해준다.&lt;/li&gt;
  &lt;li&gt;Data 에 내재하는 선형적이고 단순한 관계 뿐 아니라 비선형적인 복잡한 관계까지 뽑아내는 데에 앙상블을 사용할 수 있다.&lt;br /&gt;
2 개의 다른 모델을 사용해서 하나의 앙상블을 구성하면 된다.(?)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;32-단점&quot;&gt;3.2 단점&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;앙상블은 모델의 해석력(interpretability)을 떨어뜨리고, 결국 비지니스 insight 를 이끌어내기가 굉장히 힘들어 질 수 있다.&lt;/li&gt;
  &lt;li&gt;시간이 많이 걸리고, 실시간 어플리케이션 용으로는 적합하지 않을 수 있다.&lt;/li&gt;
  &lt;li&gt;앙상블을 구성하는 모델을 잘 선택하는 방법을 익히는 것이 어렵다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-예제&quot;&gt;4. 예제&lt;/h2&gt;
&lt;p&gt;&lt;i&gt;생략&lt;/i&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-더-알아보기&quot;&gt;5. 더 알아보기&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2015/09/questions-ensemble-modeling/&quot;&gt;5 Easy questions on Ensemble Modeling everyone should know&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/02/40-questions-to-ask-a-data-scientist-on-ensemble-modeling-techniques-skilltest-solution/&quot;&gt;40 Questions to ask a Data Scientist on Ensemble Modeling Techniques (Skilltest Solution)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>CRF(Conditional Random Field)</title>
   <link href="http://localhost:4000/study/2017/11/30/conditional-random-filed/"/>
   <updated>2017-11-30T17:00:00+00:00</updated>
   <id>http://localhost:4000/study/2017/11/30/conditional-random-filed</id>
   <content type="html">&lt;h2 id=&quot;crf-란&quot;&gt;CRF 란?&lt;/h2&gt;
&lt;p&gt;저스틴 비버의 하루 일상을 순서대로 찍은 사진들이 있다고 상상해보자.&lt;br /&gt;
우리는 각각의 사진에 한 단어로 설명(라벨)을 달고자 한다. (예&amp;gt; 식사 사진, 수면 사진, 운전 중 등등)&lt;br /&gt;
어떻게 하면 될까?&lt;/p&gt;

&lt;p&gt;한가지 방법은 사진들이 가지는 순서에 대한 정보는 무시하고 각 사진 자체만으로 분류를 해보는 것이 될 수 있겠다.
예를 들어 라벨이 달린 한 달 치의 사진이 주어졌다고 하면, 아침 6시 쯤에 찍힌 어두운 사진들은 ‘수면’ 과 연관이 있는 사진들이고,
밝은 색깔이 많이 포함된 사진은 ‘춤’ 에 대한 사진, 자동차가 나온 사진들은 ‘운전’ 사진이라는 등의 사실을 배울 수 있을 것이다.&lt;/p&gt;

&lt;p&gt;하지만 순서에 대한 관점을 무시하면 많은 양의 정보를 잃게 된다. 예를 들어 입만 크게 나온 사진이 있다고 하자. 그건 노래하는 사진일까 밥 먹는 사진일까?
만약 이전의 사진이 저스틴 비버가 밥을 먹거나 요리하는 사진이었다면, 그 입 사진은 ‘먹는’ 행위에 관한 사진일 가능성이 더 높을 것이다.
하지만 이전 사진이 저스틴 비버가 노래나 춤 추는 사진이었다면, 입 사진도 마찬가지로 ‘노래하는’ 사진일 가능성이 높다.&lt;/p&gt;

&lt;h3 id=&quot;part-of-speech-tagging&quot;&gt;Part-of-Speech Tagging&lt;/h3&gt;
&lt;p&gt;part-of-speech tagging 를 예로 좀 더 자세히 살펴보도록 하자.&lt;br /&gt;
POS tagging 의 목적은 한 문장의 어휘들의 품사를 달아주는 것이다. (ex&amp;gt; 명사, 동사, 형용사, 명사)&lt;br /&gt;
예를 들어 ‘Bob drank coffee at Starbucks.’ 라는 문장은 ‘Bob(명사) drank(동사) coffee(명사) at(전치사) Starbucks(명사)’ 가 될 것이다.
그럼 이제 CRF 를 이용해서 POS tagging 을 해보자. 먼저 다른 classifier 들과 마찬가지로 feature function &lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt; 들을 정의해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;feature-functions-in-a-crf&quot;&gt;Feature functions in a CRF&lt;/h3&gt;
&lt;p&gt;CRF 에서 각각의 feature function 은 아래와 같은 입력들을 받는 함수이다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;s : 문장&lt;/li&gt;
  &lt;li&gt;i : 단어의 문장 내 인덱스&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;l_i&lt;/script&gt; : 현재 단어의 라벨&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;l_{i-1}&lt;/script&gt; : 이전 단어의 라벨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 출력 값은 실수 값이 된다.(하지만 실제로는 0 또는 1 인 경우가 많다.)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;참고 : 본 포스팅에서는 문장 전체에 대한 라벨이 아니라 현재 단어와 이전 단어의 라벨만 사용하도록 제한하고 있다.
이것은 &lt;strong&gt;linear-chain CRF&lt;/strong&gt; 의 일종이라고 할 수 있다. 설명의 편의를 위해 범용적인 CRF 에 대한 내용은 생략하고자 한다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;feature-를-probabilties-로&quot;&gt;Feature 를 Probabilties 로.&lt;/h3&gt;
&lt;p&gt;이제 각각의 feature function &lt;script type=&quot;math/tex&quot;&gt;f_j&lt;/script&gt; 에 weight &lt;script type=&quot;math/tex&quot;&gt;\lambda_j&lt;/script&gt; 를 할당해 보자.(데이터로부터 이 weight 들을 어떻게 학습하는 지는 아래에 설명하겠다.)&lt;br /&gt;
문장 s 가 주어졌을 때, 문장 내 모든 단어에 대한 weighted feature 값을 더하여 s 에 라벨 l 이 부여될 수 있는지 점수를 매길 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;score(l|s) = \sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})&lt;/script&gt;

&lt;p&gt;(첫 번째 시그마는 각각의 feature function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; 에 대한 것이고, 안쪽 시그마는 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; 위치의 단어 각각에 대한 연산이다.)&lt;/p&gt;

&lt;p&gt;그 다음 지수 연산(exponentiating)과 정규화(normalizing)를 통해 이 점수를 확률
&lt;script type=&quot;math/tex&quot;&gt;p(l | s)&lt;/script&gt; 로 환산할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(l | s) = \frac{exp[score(l|s)]}{\sum_{l’} exp[score(l’|s)]} = \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})]}{\sum_{l’} exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l’_i, l’_{i-1})]}&lt;/script&gt;

&lt;h3 id=&quot;feature-function-예시&quot;&gt;Feature Function 예시&lt;/h3&gt;
&lt;p&gt;그럼 이 feature function 들은 어떻게 생겼을까? POS tagging 의 feature 들은 다음과 같을 수 있다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_1(s, i, l_i, l_{i-1}) =
\begin{cases}
 1 &amp; \mbox{if }l_i = \text{부사(ADVERB)} \mbox{ and the ith word ends in “-ly”} \\
 0 &amp; \mbox{otherwise.}
 \end{cases} %]]&gt;&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;이 feature 에 대한 weight &lt;script type=&quot;math/tex&quot;&gt;\lambda_1&lt;/script&gt; 값이 양수이고 큰 경우, 이 feature 는 -ly 로 끝나는 단어를 ‘부사’ 로 라벨링할 거라는 의미가 될 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_2(s, i, l_i, l_{i-1}) =
\begin{cases}
1 &amp; \mbox{if } i = 1, l_i = 동사(VERB), \mbox{ and the sentence ends in a question mark} \\
0 &amp; \mbox{otherwise.}
\end{cases} %]]&gt;&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;마찬가지로 feature 에 대한 weight &lt;script type=&quot;math/tex&quot;&gt;\lambda_2&lt;/script&gt; 값이 양수이고 큰 경우, ? 로 끝나는 문장의 첫 단어를 ‘동사’ 로 라벨링하고자 할 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_3(s, i, l_i, l_{i-1}) =
\begin{cases}
1 &amp; \mbox{if }l_{i-1} = \mbox{형용사(ADJECTIVE)} \mbox{, and }l_i = \mbox{명사(NOUN)} \\
0 &amp; \mbox{otherwise.}
\end{cases} %]]&gt;&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;weight 가 양수 인 경우, 이 feature 는 형용사 뒤에는 명사가 온다는 것을 의미한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;f_4(s, i, l_i, l_{i-1}) = 1 \mbox{ if }l_{i-1} =\mbox{ 전치사(PREPOSITION) and }l_i = \mbox{전치사(PREPOSITION).}&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;이 함수에 대한 음수의 weight &lt;script type=&quot;math/tex&quot;&gt;\lambda_4&lt;/script&gt; 는 전치사 뒤에는 전치시가 오면 안되다는 것을 의미한다. 따라서 그런 라벨링이 발생하지 않도록 할 수 있을 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이게 끝이다!&lt;br /&gt;
정리해보면 CRF 를 만들기 위해서는,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;feature function 몇 개를 정의해주고, (feature function 은 전체 문장, 현재 위치, 그리고 주변의 라벨 값을 참조한다.)&lt;/li&gt;
  &lt;li&gt;weight 를 할당하고&lt;/li&gt;
  &lt;li&gt;전체를 몽땅 더한 다음,&lt;/li&gt;
  &lt;li&gt;필요하면 그 값을 확률로 환산하면 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이제 CRF 이 다른 머신 러닝 기술들와 어떻게 다른지 비교해보도록 하자.&lt;/p&gt;

&lt;h2 id=&quot;smells-like-logistic-regression&quot;&gt;Smells like Logistic Regression…&lt;/h2&gt;
&lt;p&gt;CRF 의 아래 식은 왠지 친숙해 보일 수도 있다. &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot; target=&quot;_blank&quot;&gt;궁금하면 여기&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(l | s) = \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n f_j(s, i, l_i, l_{i-1})]}{\sum_{l’} exp[\sum_{j = 1}^m \sum_{i = 1}^n f_j(s, i, l’_i, l’_{i-1})]}&lt;/script&gt;

&lt;p&gt;왜냐하면 CRF 는 근본적으로 Logistic Regression 의 순차 버전(sequential version)이기 때문이다.&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;Logistic Regression 이 분류(claasification) 을 위한 log-linear 모델인데 반해,&lt;br /&gt;
CRF 는 순차적인 라벨링(sequential labels) 을 위한 log-linear 모델이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;looks-like-hmms&quot;&gt;Looks like HMMs…&lt;/h2&gt;
&lt;p&gt;HMM(&lt;a href=&quot;http://en.wikipedia.org/wiki/Hidden_Markov_model&quot; target=&quot;_blank&quot;&gt;Hidden Markov Model&lt;/a&gt;) 도 POS tagging(또한 일반적인 순차 라벨링) 에 사용될 수 있는 모델임을 기억하자.&lt;br /&gt;
CRF 가 라벨링 점수를 계산하기 위해 여러 개의 feature function 을 다 같이 사용하는데 비해,&lt;br /&gt;
HMM 은 라벨링을 위해 생성적(generative) 방식을 취한다. 그 식은 아래와 같다:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(l,s) = p(l_1) \prod_i p(l_i | l_{i-1}) p(w_i | l_i)&lt;/script&gt;

&lt;p&gt;이 때,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$
p(l_i | l_{i-1})$ 는 전이 확률(transition probabilities) - 예&amp;gt; 전치사 뒤에 명사가 나올 확률&lt;/li&gt;
  &lt;li&gt;$
p(w_i | l_i)$ 는 출력 확률(emission probabilities) - 예&amp;gt; ‘명사’ 라벨이 ‘아빠’ 를 도출할 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그럼 HMM 과 CRF 는 어떻게 다른가?&lt;br /&gt;
&lt;strong&gt;CRF 가 더 강력하다!&lt;/strong&gt;&lt;br /&gt;
CRF 는 HMM 이 할 수 있는 모든 것을 모델링할 수 있고, 그것보다 더 많은 일을 해낸다. 왜 그런지는 아래를 살펴보자.&lt;/p&gt;

&lt;p&gt;HMM 확률 식에 log 를 취하면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log p(l,s) = \log p(l_0) + \sum_i \log p(l_i | l_{i-1}) + \sum_i \log p(w_i | l_i)&lt;/script&gt;

&lt;p&gt;이 log-확률을 2진(binary) 전이, 출력 feature 들에 할당된 weight 라고 간주하면, 위 식은 CRF 의 log-linear 형태와 완전히 일치한다.
다시 말해, 어떠한 HMM 이라도 그와 동일한 CRF 를 구현할 수 있다는 뜻이다. 그 방법은 아래와 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;HMM 의 전이 확률 $p(l_i = y | l_{i-1} = x)$ 각각에 대해 CRF 전이 feature 를 아래처럼 정의한다.&lt;br /&gt;
$f_{x,y}(s, i, l_i, l_{i-1}) = 1 \text{ if }l_i = y \text{, and }l_{i-1} = x$&lt;br /&gt;
그리고 각 feature 에 다음과 같은 weight 를 부여한다.&lt;br /&gt;
$w_{x,y} = \log p(l_i = y | l_{i-1} = x)$&lt;/li&gt;
  &lt;li&gt;위와 유사하게, HMM 의 출력 확률 $p(w_i = z | l_{i} = x)$ 각각에 대해 CRF 출력 feature 를 아래처럼 정의한다.&lt;br /&gt;
$g_{x,y}(s, i, l_i, l_{i-1}) = 1 \text{ if }w_i = z\text{, and }l_i = x$&lt;br /&gt;
그리고 각 feature 에 다음과 같은 weight 를 부여한다.&lt;br /&gt;
$w_{x,z} = \log p(w_i = z | l_i = x)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러면 이 feature function 들을 사용하여 계산된 &lt;script type=&quot;math/tex&quot;&gt;p(l|s)&lt;/script&gt; 점수는 해당하는 HMM 에 의해 계산된 점수에 정확히 비례하게 되고, 따라서 모든 HMM 은 특정 CRF 로 표현이 가능해진다.&lt;/p&gt;

&lt;p&gt;하지만 CRF 훨씬 풍부한 라벨 분포 set 를 모델링할 수 있는데, 이 이유는 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CRF 는 훨씬 많은 수의 feature set 를 정의할 수 있다.&lt;/strong&gt; HMM 은 태생적으로 지엽적(local) 속성을 가진다.
왜냐면 HMM 은 2진 전이, 출력 feature function 만 사용 가능하고, 이로 인해 각 단어는 오직 현재의 라벨에, 그리고 각 라벨은 바로 이전의 라벨에만 의존하게 되기 때문이다.
이에 반해 CRF 는 좀더 전역적(global) feature 들을 사용할 수 있다. 예를 들어 위에서 보여준 POS tagger 의 feature 들 중 하나는 &lt;em&gt;문장의 끝이 ? 인 경우
문장의 제일 첫 단어가 동사&lt;/em&gt; 로 태깅될 확률을 높이고 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CRF 는 임의(arbitrary)의 weight 값들을 가질 수 있다.&lt;/strong&gt; HMM 의 확률 값들은 어떤 조건 - 예를 들면,  $
0 &amp;lt;= p(w_i | l_i) &amp;lt;= 1, \sum_w p(w_i = w | l_1) = 1$ - 을 반드시 충족시켜야만 한다. 그에 비해 CRF 의 weight 는 아무 제약이 없다.&lt;br /&gt;
$\log p(w_i | l_i)$ 는 어떠한 값이든 될 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;weight-의-학습&quot;&gt;Weight 의 학습&lt;/h3&gt;
&lt;p&gt;CRF 에서 weight 를 어떻게 학습시키는지에 대한 질문으로 다시 되돌아가자. &lt;strong&gt;gradient ascent (짜잔)&lt;/strong&gt; 를 사용하는 것이 하나의 방법이 될 수 있다.&lt;/p&gt;

&lt;p&gt;학습 데이터(문장들과 그에 대한 POS 라벨들)가 충분히 있다고 가정해보자. 먼저 CRF 모델의 weight 들을 임의의 값으로 초기화 한다.
이 무작위 weight 값들을 정답으로 바꿔나가려면 각각의 학습 데이터에 다음과 같은 작업을 해주면 된다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;각각의 feature function &lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt; 에서 학습 데이터의 log 확률에 대한 gradient 를 계산한다.
&lt;script type=&quot;math/tex&quot;&gt;\lambda_i \text{ : }\frac{\partial}{\partial w_j} \log p(l | s) = \sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l’} p(l’ | s) \sum_{j = 1}^m f_i(s, j, l’_j, l’_{j-1})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;gradient 의 첫 번째 term 은 &lt;em&gt;true&lt;/em&gt; 라벨을 갖는 feature &lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt; 의 분포를,
 두 번째 term 은 현재 모델에서 feature &lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt; 의 &lt;em&gt;expected&lt;/em&gt; 값의 분포를 나타냄을 주목하자.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; 를 gradient 방향으로 이동시킨다 :&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\lambda_i = \lambda_i + \alpha [\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l’} p(l’ | s) \sum_{j = 1}^m f_i(s, j, l’_j, l’_{j-1})]&lt;/script&gt; where $\alpha$ is some learning rate.&lt;/li&gt;
  &lt;li&gt;특정 조건(더 이상 갱신이 안된다던가)에 도달할 때까지 위의 작업을 반복한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;최적-라벨-찾기&quot;&gt;최적 라벨 찾기&lt;/h3&gt;
&lt;p&gt;우리의 CRF 모델을 학습시켰고 이제 새로운 문장이 입력된다고 생각해보자. 라벨링은 어떻게 되나?&lt;/p&gt;

&lt;p&gt;단순한 방법은 모든 가능한 라벨 l 에 대한 &lt;script type=&quot;math/tex&quot;&gt;p(l | s)&lt;/script&gt; 를 계산하고, 이 확률을 최대화 시키는 라벨을 고르는 것이 될테다.
하지만 m 길이의 문장과 k 개의 tag 셋이 있다면 총 &lt;script type=&quot;math/tex&quot;&gt;k^m&lt;/script&gt; 개의 조합이 가능하기 때문에, 이 방식은 지수승의 연산을 필요로 한다.&lt;/p&gt;

&lt;p&gt;더 좋은 방법이 있다. (linear-chain) CRF 는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Optimal_substructure&quot; target=&quot;_blank&quot;&gt;optimal substructure&lt;/a&gt; 속성을 만족하기 때문에, (polynomial-time 복잡도의) 동적 프로그래밍 알고리즘으로 최적 라벨을 찾을 수 있다. 이는 HMM 에 &lt;a href=&quot;http://en.wikipedia.org/wiki/Viterbi_algorithm&quot; target=&quot;_blank&quot;&gt;Viterbi 알고리즘&lt;/a&gt;을 적용하는 것과 유사하다.&lt;/p&gt;

&lt;h3 id=&quot;좀-더-재밌는-응용&quot;&gt;좀 더 재밌는 응용&lt;/h3&gt;
&lt;p&gt;사실 part-of-speech tagging 은 좀 지루하고, 다른 POS 태거도 이미 많다. 실 생활에서 CRF 는 어디다가 써먹을 수 있을까?&lt;/p&gt;

&lt;p&gt;트위터에서 사람들이 크리스마스에 받은 선물의 타입을 마이닝하고 싶다. 고 해보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/CRF_Twitter.jpg&quot; alt=&quot;twitter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러면 문장에서 어느 단어가 선물에 해당하는지 어떻게 찾아낼 수 있나??!!1&lt;/p&gt;

&lt;p&gt;위와 같은 그래프를 만들기 위해 나는 그냥 “크리스마스에 XXX 가 갖고싶어요.” 와 “크리스마스에 XXX 를 받았어요!” 형태의 문장을 검색했을 뿐이다.
하지만 더 세련된 CRF 버전으로, part-of-speech 에서의 품사 같이 GIFT 라는 태그 (한 발 더 나가면 GIFT-GIVER, GIFT-RECEIVER 를 이용해
  선물을 준사람과 받은사람의 정보까지) 를 사용해서 이것을 POS 태깅 문제처럼 해결해 볼 수 있을 것이다. 그 때 feature 는,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“이전 단어가 GIFT-RECEIVER 이고  그 이전 단어가 ‘gave’ 라면, 현재 단어는 GIFT 임”&lt;/li&gt;
  &lt;li&gt;“다음 두 단어가 ‘for Christmas’ 면 현재 단어는 GIFT 임”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;와 같은 형태가 될 수 있을 것이다.&lt;/p&gt;

&lt;h3 id=&quot;마무리&quot;&gt;마무리&lt;/h3&gt;
&lt;p&gt;좀더 ‘랜덤한’ 생각을 나열하고 끝내고자 한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;이 포스트에서는 CRF 를 사용한 그래픽 모델 프레임웍에 대한 내용은 일부러 스킵했다. 왜냐면 CRF 의 기본을 이해하는데는 큰 도움이 되지 않기 때문이다.
하지만 더 알고싶다면 Daphne Koller 가 공짜로 가르쳐주는 &lt;a href=&quot;http://www.pgm-class.org&quot; target=&quot;_blank&quot;&gt;온라인 강좌&lt;/a&gt;를 들어보시길.&lt;/li&gt;
  &lt;li&gt;아니면 CRF 를 적용한 많은 NLP 응용 분야(POS 태깅이나 &lt;a href=&quot;http://en.wikipedia.org/wiki/Named-entity_recognition&quot; target=&quot;_blank&quot;&gt;named entity extraction&lt;/a&gt;등)에 관심이 있다면
Manning 과 Jurafsky 가 강의를 하고 있다. (링크가 있었는데 깨짐. http://www.nlp-class.org)&lt;/li&gt;
  &lt;li&gt;또한 CRF/HMM 과 Logistic Regression/Naive Bayes 간의 유사점에 대해 조금 더 꾸며봤다. 아래 이미지를 참고바람.
&lt;img src=&quot;/public/img/graph_model.png&quot; alt=&quot;graph_model&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;출처 : &lt;a href=&quot;http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields&quot; target=&quot;_blank&quot;&gt;introduction to conditional random fields&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>jsMath(TeX) 주요 문법</title>
   <link href="http://localhost:4000/study/jsmath/tex/2017/11/29/js-math/"/>
   <updated>2017-11-29T21:10:00+00:00</updated>
   <id>http://localhost:4000/study/jsmath/tex/2017/11/29/js-math</id>
   <content type="html">&lt;h2 id=&quot;jsmathtex-in-markdown&quot;&gt;jsMath(TeX) in markdown&lt;/h2&gt;

&lt;p&gt;마크다운 문서에서는 아래와 같이 $$ 부호 내에 입력하면 자동 전환됨.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$$
y = x^2
$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;결과 :&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;y = x^2&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-산술-함수&quot;&gt;1. 산술 함수&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TeX&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;\exp_a b = a^b&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\exp_a b = a^b&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\exp b = e^b&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\exp b = e^b&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10^m, \ln c, \lg d = \log e&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;10^m, \ln c, \lg d = \log e&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\log_{10} f&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\log_{10} f&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\min(x,y), \max(x,y)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\min(x,y), \max(x,y)&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;2-미분&quot;&gt;2. 미분&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TeX&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;dt, \partial t, {dy \over dx}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;dt, \partial t, {dy \over dx}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;{\partial^2\over\partial x_1\partial x_2}y&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;{\partial^2\over\partial x_1\partial x_2}y&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;f’, f’’, f^{(3)}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;f', f'', f^{(3)}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\dot y&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\dot y&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;3-근호&quot;&gt;3. 근호&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TeX&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;\sqrt{2}, \sqrt[n]{x^5}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\sqrt{2}, \sqrt[n]{x^5}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\sqrt[3]{x^3+y^3 \over 2}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\sqrt[3]{x^3+y^3 \over 2}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;4-순열과-조합&quot;&gt;4. 순열과 조합&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TeX&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;{}&lt;em&gt;nP&lt;/em&gt;{k}, &lt;em&gt;{n}C&lt;/em&gt;{k}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;{}_nP_{k}, \; _{n}C_{k}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;^{n}P_{k}, P_{k}^{n}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;^{n}P_{k}, \; P_{k}^{n}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;C(n,k)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;C(n,k)&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;5-집합&quot;&gt;5. 집합&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TeX&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;{ }&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\{ \}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\in, \notin, \ni, \not\ni&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\in, \notin, \ni, \not\ni&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\cap, \cup, \subset, \supset&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\cap, \cup, \subset, \supset&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;6-위-아래-전치-후치-첨자&quot;&gt;6. 위, 아래, 전치, 후치, 첨자&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TeX&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;y_m,   a_{i,j},   x_2^3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;y_m, \; a_{i,j}, \; x_2^3&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;{}&lt;em&gt;{b}^{a}X,   _{c}^{a}C&lt;/em&gt;{d}^{b}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;{}_{b}^{a}X, \; _{c}^{a}C_{d}^{b}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\overset{x}{P},   \underset{y}{Z}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\overset{x}{P}, \; \underset{y}{Z}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;7-기타&quot;&gt;7. 기타&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th&gt;TeX&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;공백&lt;/td&gt;
      &lt;td&gt;a \quad b, a\ b&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;a \quad b, a\ b&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;색깔&lt;/td&gt;
      &lt;td&gt;{\color{Blue}x^2}+{\color{Red}2x}-{\color{Green}1}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;{\color{Blue}x^2}+{\color{Red}2x}-{\color{Green}1}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;한글&lt;/td&gt;
      &lt;td&gt;\text{한 글}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{한 글}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;괄호&lt;/td&gt;
      &lt;td&gt;\left( \frac{1}{2} \right)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\left( \frac{1}{2} \right)&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;집합 괄호&lt;/td&gt;
      &lt;td&gt;\left\{ A \right\}&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\left\{ A \right\}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;시그마&lt;/td&gt;
      &lt;td&gt;\sum_{k=1}^N k^2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum_{k=1}^N k^2&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;곱기호&lt;/td&gt;
      &lt;td&gt;\prod_{i=1}^N x_i&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\prod_{i=1}^N x_i&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;극한&lt;/td&gt;
      &lt;td&gt;\lim_{n \to \infty}x_n&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\lim_{n \to \infty}x_n&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;적분&lt;/td&gt;
      &lt;td&gt;\int_{-N}^{N} e^x\, dx&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\int_{-N}^{N} e^x\, dx&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;f(n)=
\begin{cases}
n/2, &amp;amp; \mbox{if }n\mbox{ is even} \\
3n+1, &amp;amp; \mbox{if }n\mbox{ is odd}
\end{cases}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
f(n)=
\begin{cases}
n/2, &amp; \mbox{if }n\mbox{ is even} \\
3n+1, &amp; \mbox{if }n\mbox{ is odd}
\end{cases} %]]&gt;&lt;/script&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Gibbs Sampling</title>
   <link href="http://localhost:4000/study/2017/11/29/gibbs-sampling/"/>
   <updated>2017-11-29T20:55:00+00:00</updated>
   <id>http://localhost:4000/study/2017/11/29/gibbs-sampling</id>
   <content type="html">&lt;h2 id=&quot;정의&quot;&gt;정의&lt;/h2&gt;
&lt;p&gt;In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables). Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.&lt;/p&gt;

&lt;p&gt;Gibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference. It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM).&lt;/p&gt;

&lt;p&gt;As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired. Generally, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution and are usually discarded. If necessary, one possible remedy is thinning the resulting chain of samples (i.e. only taking every nth value, e.g. every 10th value). It has been shown, however, that using a longer chain instead (e.g. a chain that is n times as long as the initially considered chain using a thinning factor of n) leads to better estimates of the true posterior. Thus, thinning should only be applied when time or computer memory are restricted.[1]&lt;/p&gt;

&lt;p&gt;hello
&lt;script type=&quot;math/tex&quot;&gt;y = \frac{x ^ 2}{2} K\left(1-\sqrt{x^3 - 8}\right)&lt;/script&gt;
world&lt;/p&gt;

&lt;p&gt;출처 : &lt;a href=&quot;https://en.wikipedia.org/wiki/Gibbs_sampling#Introduction&quot; target=&quot;_blank&quot;&gt;Gibbs Sampling&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Monte Carlo Method</title>
   <link href="http://localhost:4000/study/2017/11/29/Monte-Carlo-Method/"/>
   <updated>2017-11-29T20:00:00+00:00</updated>
   <id>http://localhost:4000/study/2017/11/29/Monte-Carlo-Method</id>
   <content type="html">&lt;h2 id=&quot;정의&quot;&gt;정의&lt;/h2&gt;
&lt;p&gt;난수를 이용하여 함수의 값을 확률적으로 계산하는 알고리즘.&lt;/p&gt;
&lt;h4 id=&quot;핵심-아이디어&quot;&gt;핵심 아이디어&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;이론상 determistic 할지도 모르는 문제에 대해 randomness 를 사용하여 해결한다!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;주로 물리/수학적 문제들에 대해 다른 방법으로 해결할 수 없는 경우에 사용한다.(ㅋㅋ)&lt;/p&gt;

&lt;h4 id=&quot;주요-적용-분야&quot;&gt;주요 적용 분야&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;최적화 문제&lt;/li&gt;
  &lt;li&gt;numerical integration (수치적 미분?)&lt;/li&gt;
  &lt;li&gt;확률 분포로부터 draw 를 생성 (generating draw from probability distribution)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;어떻게-동작하나&quot;&gt;어떻게 동작하나?&lt;/h4&gt;
&lt;p&gt;일반적으로 아래 패턴으로 동작함.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;입력 가능한 데이터의 도메인을 정의한다.&lt;/li&gt;
  &lt;li&gt;해당 도메인에 대한 확률 분포로부터 랜덤 데이터를 생성&lt;/li&gt;
  &lt;li&gt;입력 데이터에 대한 deterministic 계산을 수행&lt;/li&gt;
  &lt;li&gt;결과 수집&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;예를 들어, (x,y) 그래프의 1 사분면에 (0, 0) 을 중점으로 하고 반지름이 1 인 원이 있다고 생각해보자. 원의 넓이는 π/4 일 것이다.&lt;br /&gt;
이 때 Monte Carlo Method 를 이용해 π 값을 근사해 보면,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;한 변의 길이가 1인 정사각형을 그리고, 그 안에 반지름이 1 인 원을 그린다. (1/4 원)&lt;/li&gt;
  &lt;li&gt;균등한 크기의 물체를 사각형 안에 균등하게 흩뿌린다.(uniformly scatter)&lt;/li&gt;
  &lt;li&gt;원 안에 들어간 물체의 갯수와 전체 갯수를 센다.&lt;/li&gt;
  &lt;li&gt;원 안에 들어간 물체의 갯수와 전체 갯수의 비율은 두 영역의 비율과 같으므로, 즉 π/4 가 된다.&lt;br /&gt;
해당 비율에 4을 곱해서 π 의 근사치를 구할 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;여기서 중요한 포인트 2가지.&lt;/p&gt;
&lt;blockquote&gt;

  &lt;ol&gt;
    &lt;li&gt;흩뿌릴 때 균등하게 분포되지 않으면 결과가 좋지 않을 것이다.&lt;/li&gt;
    &lt;li&gt;입력 데이터의 갯수가 많아야 한다. 많을 수록 결과가 개선될 것이다.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/monte.gif&quot; alt=&quot;monte carlo method&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Monte Carlo Method 를 적용하기 위해서는 많은 수의 랜덤 값들이 요구되는데, 이로 인하여 pseudorandom number generator 의 개발에 박차를 가하게 되었다. pseudorandom number generator 는 기존에 통계적 샘플링에서 주로 사용되던 랜덤 숫자 테이블(table of random numbers) 대비 훨씬훨씬 성능이 좋았다.&lt;/p&gt;

&lt;p&gt;출처 : &lt;a href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_method&quot; target=&quot;_blank&quot;&gt;Monte Carlo method&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Tips for Mongodb</title>
   <link href="http://localhost:4000/mongoose/study/2017/11/25/mongo-tips/"/>
   <updated>2017-11-25T10:30:00+00:00</updated>
   <id>http://localhost:4000/mongoose/study/2017/11/25/mongo-tips</id>
   <content type="html">&lt;h1 id=&quot;useful-queries&quot;&gt;Useful queries&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;$ne&lt;/strong&gt;&lt;br /&gt;
desc != ''
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{ desc: { $ne : '' } }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;$in&lt;/strong&gt;,  &lt;strong&gt;$nin&lt;/strong&gt;&lt;br /&gt;
desc is in [] (or not in [])
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{ desc: { $nin : ['', null] } }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;$regex&lt;/strong&gt; : 정규 표현식&lt;br /&gt;
해당 단어를 포함하는지 확인 가능
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{ 'title' : { $regex : 'shaver' } }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;data-export--import&quot;&gt;Data export / import&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;export
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mongoexport -h 127.0.0.1:27000 -d dbName -c collectionName -o filename.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;import
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mongoimport -h 127.0.0.1:27017 -d dbName -c collectionName --file filename.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Markdown syntax</title>
   <link href="http://localhost:4000/study/markdown/2017/11/23/markdown-syntax/"/>
   <updated>2017-11-23T12:30:00+00:00</updated>
   <id>http://localhost:4000/study/markdown/2017/11/23/markdown-syntax</id>
   <content type="html">&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#headers&quot;&gt;Headers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#block-quote&quot;&gt;Block Quote&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#list&quot;&gt;List&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#ordered-list&quot;&gt;Ordered List&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#unordered-list&quot;&gt;Unordered List&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#code&quot;&gt;Code&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#hr&quot;&gt;수평선&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#links&quot;&gt;Links&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#emphasis&quot;&gt;강조&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#images&quot;&gt;Images&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;1-headers-&quot;&gt;1. Headers &lt;a name=&quot;headers&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# This is a H1
## This is a H2
### This is a H3
#### This is a H4
##### This is a H5
###### This is a H6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;this-is-a-h1&quot;&gt;This is a H1&lt;/h1&gt;
&lt;h2 id=&quot;this-is-a-h2&quot;&gt;This is a H2&lt;/h2&gt;
&lt;h3 id=&quot;this-is-a-h3&quot;&gt;This is a H3&lt;/h3&gt;
&lt;h4 id=&quot;this-is-a-h4&quot;&gt;This is a H4&lt;/h4&gt;
&lt;h5 id=&quot;this-is-a-h5&quot;&gt;This is a H5&lt;/h5&gt;
&lt;h6 id=&quot;this-is-a-h6&quot;&gt;This is a H6&lt;/h6&gt;

&lt;h1 id=&quot;2-block-quote-&quot;&gt;2. Block Quote &lt;a name=&quot;block-quote&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; This is a blockquote.
&amp;gt;&amp;gt; This is a blockquote.
&amp;gt;&amp;gt;&amp;gt; This is a blockquote.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is a blockquote.&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;This is a blockquote.&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This is a blockquote.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;3-list-&quot;&gt;3. List &lt;a name=&quot;list&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;3-1-ordered-list-&quot;&gt;3-1 Ordered list &lt;a name=&quot;ordered-list&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. 첫번째
2. 두번째
3. 세번째
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;첫번째&lt;/li&gt;
  &lt;li&gt;두번째&lt;/li&gt;
  &lt;li&gt;세번째&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3-2-unordered-list-&quot;&gt;3-2 Unordered list &lt;a name=&quot;unordered-list&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* 빨강
  * 녹색
    * 파랑

+ 빨강
  + 녹색
    + 파랑

- 빨강
- 녹색
- 파랑
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;빨강
    &lt;ul&gt;
      &lt;li&gt;녹색
        &lt;ul&gt;
          &lt;li&gt;파랑&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;빨강
    &lt;ul&gt;
      &lt;li&gt;녹색
        &lt;ul&gt;
          &lt;li&gt;파랑&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;빨강&lt;/li&gt;
  &lt;li&gt;녹색&lt;/li&gt;
  &lt;li&gt;파랑&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;4-code-&quot;&gt;4. code &lt;a name=&quot;code&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;` 3개 연속으로 입력 후 코드 입력하면 됨. (코드 끝나면 ` 3개)&lt;/p&gt;

&lt;p&gt;```&lt;br /&gt;
print(‘hello world’)&lt;br /&gt;
```&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print('hello world')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;5-수평선-&quot;&gt;5. 수평선 &lt;a name=&quot;hr&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;아래 줄은 모두 수평선을 만든다. 마크다운 문서를 미리보기로 출력할 때 페이지 나누기 용도로 많이 사용한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* * *
또는,
***
또는,
*****
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;6-links-&quot;&gt;6. Links &lt;a name=&quot;links&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[link keyword][id]
[id]: URL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;http://m.naver.com&quot;&gt;url&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;새 탭에서 여는 방법:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[link keyword][id]{:target='_blank'}
[id]: URL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;http://m.naver.com&quot; target=&quot;_blank&quot;&gt;new tab&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;인라인 링크 : [title](link url)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;http://www.naver.com&quot;&gt;inline link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;7-강조-&quot;&gt;7. 강조 &lt;a name=&quot;emphasis&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;*single asterisks*
_single underscores_
**double asterisks**
__double underscores__
++underline++
~~cancelline~~
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;single asterisks&lt;/em&gt;&lt;br /&gt;
&lt;em&gt;single underscores&lt;/em&gt;&lt;br /&gt;
&lt;strong&gt;double asterisks&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;double underscores&lt;/strong&gt;&lt;br /&gt;
++underline++&lt;br /&gt;
&lt;del&gt;cancelline&lt;/del&gt;&lt;/p&gt;

&lt;h1 id=&quot;8-images-&quot;&gt;8. Images &lt;a name=&quot;images&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;![Alt text](/public/img/machine.jpg)
![Alt text](/public/img/machine.jpg &quot;Optional title&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/maschine.jpg&quot; alt=&quot;&amp;quot;maschine&amp;quot;&quot; title=&quot;maschine&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Restricted Boltzmann Machines</title>
   <link href="http://localhost:4000/study/machinelearning/2017/11/22/rectricted-boltzmann-machine/"/>
   <updated>2017-11-22T16:00:00+00:00</updated>
   <id>http://localhost:4000/study/machinelearning/2017/11/22/rectricted-boltzmann-machine</id>
   <content type="html">&lt;h2 id=&quot;제한된-볼츠만-머신&quot;&gt;제한된 볼츠만 머신&lt;/h2&gt;

&lt;p&gt;데이터를 재구성(reconstruct) 함을 통해 입력 데이터의 패턴을 찾아내는 모델.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Geoff Hinton 교수가 제안&lt;/li&gt;
  &lt;li&gt;Vanishing Gradient 에 대한 해결 방안으로 사용 가능&lt;/li&gt;
  &lt;li&gt;Unsupervised&lt;/li&gt;
  &lt;li&gt;Feature extractor 의 일종이라고 할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;데이터에 내재한 패턴을 찾아냄.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;auto encoder 와 유사함.&lt;/li&gt;
  &lt;li&gt;Shallow 2-layer net 이다.
    &lt;ul&gt;
      &lt;li&gt;visible layer&lt;/li&gt;
      &lt;li&gt;hidden layer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/RBM_1.jpg&quot; alt=&quot;&amp;quot;RBM&amp;quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;recontruction&quot;&gt;Recontruction&lt;/h2&gt;

&lt;p&gt;forward / backward 를 반복적으로 학습시켜 data 의 숨겨진 feature 들을 학습시킨다.&lt;br /&gt;
loss 는 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0&quot; target=&quot;_blank&quot;&gt;KL Divergence&lt;/a&gt; 를 사용.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/RBM_2.jpg&quot; alt=&quot;&amp;quot;RBM&amp;quot;&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Example content</title>
   <link href="http://localhost:4000/2014/01/01/example-content/"/>
   <updated>2014-01-01T00:00:00+00:00</updated>
   <id>http://localhost:4000/2014/01/01/example-content</id>
   <content type="html">&lt;div class=&quot;message&quot;&gt;
  Howdy! This is an example blog post that shows several types of HTML content supported in this theme.
&lt;/div&gt;

&lt;p&gt;Cum sociis natoque penatibus et magnis &lt;a href=&quot;#&quot;&gt;dis parturient montes&lt;/a&gt;, nascetur ridiculus mus. &lt;em&gt;Aenean eu leo quam.&lt;/em&gt; Pellentesque ornare sem lacinia quam venenatis vestibulum. Sed posuere consectetur est at lobortis. Cras mattis consectetur purus sit amet fermentum.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Curabitur blandit tempus porttitor. Nullam quis risus eget urna mollis ornare vel eu leo. Nullam id dolor id nibh ultricies vehicula ut id elit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Etiam porta &lt;strong&gt;sem malesuada magna&lt;/strong&gt; mollis euismod. Cras mattis consectetur purus sit amet fermentum. Aenean lacinia bibendum nulla sed consectetur.&lt;/p&gt;

&lt;h2 id=&quot;inline-html-elements&quot;&gt;Inline HTML elements&lt;/h2&gt;

&lt;p&gt;HTML defines a long list of available inline tags, a complete list of which can be found on the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTML/Element&quot;&gt;Mozilla Developer Network&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;To bold text&lt;/strong&gt;, use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;strong&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;To italicize text&lt;/em&gt;, use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;em&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Abbreviations, like &lt;abbr title=&quot;HyperText Markup Langage&quot;&gt;HTML&lt;/abbr&gt; should use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;abbr&amp;gt;&lt;/code&gt;, with an optional &lt;code class=&quot;highlighter-rouge&quot;&gt;title&lt;/code&gt; attribute for the full phrase.&lt;/li&gt;
  &lt;li&gt;Citations, like &lt;cite&gt;— Mark otto&lt;/cite&gt;, should use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;cite&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;del&gt;Deleted&lt;/del&gt; text should use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;del&amp;gt;&lt;/code&gt; and &lt;ins&gt;inserted&lt;/ins&gt; text should use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;ins&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Superscript &lt;sup&gt;text&lt;/sup&gt; uses &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;sup&amp;gt;&lt;/code&gt; and subscript &lt;sub&gt;text&lt;/sub&gt; uses &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;sub&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of these elements are styled by browsers with few modifications on our part.&lt;/p&gt;

&lt;h2 id=&quot;heading&quot;&gt;Heading&lt;/h2&gt;

&lt;p&gt;Vivamus sagittis lacus vel augue rutrum faucibus dolor auctor. Duis mollis, est non commodo luctus, nisi erat porttitor ligula, eget lacinia odio sem nec elit. Morbi leo risus, porta ac consectetur ac, vestibulum at eros.&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;Cum sociis natoque penatibus et magnis dis &lt;code class=&quot;highlighter-rouge&quot;&gt;code element&lt;/code&gt; montes, nascetur ridiculus mus.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// Example can be run directly in your JavaScript console
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Create a function that takes two arguments and returns the sum of those arguments
&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;adder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;return a + b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Call the function
&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;adder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// &amp;gt; 8&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Aenean lacinia bibendum nulla sed consectetur. Etiam porta sem malesuada magna mollis euismod. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa.&lt;/p&gt;

&lt;h3 id=&quot;lists&quot;&gt;Lists&lt;/h3&gt;

&lt;p&gt;Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aenean lacinia bibendum nulla sed consectetur. Etiam porta sem malesuada magna mollis euismod. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Praesent commodo cursus magna, vel scelerisque nisl consectetur et.&lt;/li&gt;
  &lt;li&gt;Donec id elit non mi porta gravida at eget metus.&lt;/li&gt;
  &lt;li&gt;Nulla vitae elit libero, a pharetra augue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Donec ullamcorper nulla non metus auctor fringilla. Nulla vitae elit libero, a pharetra augue.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Vestibulum id ligula porta felis euismod semper.&lt;/li&gt;
  &lt;li&gt;Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.&lt;/li&gt;
  &lt;li&gt;Maecenas sed diam eget risus varius blandit sit amet non magna.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cras mattis consectetur purus sit amet fermentum. Sed posuere consectetur est at lobortis.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;HyperText Markup Language (HTML)&lt;/dt&gt;
  &lt;dd&gt;The language used to describe and define the content of a Web page&lt;/dd&gt;

  &lt;dt&gt;Cascading Style Sheets (CSS)&lt;/dt&gt;
  &lt;dd&gt;Used to describe the appearance of Web content&lt;/dd&gt;

  &lt;dt&gt;JavaScript (JS)&lt;/dt&gt;
  &lt;dd&gt;The programming language used to build advanced Web sites and applications&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Integer posuere erat a ante venenatis dapibus posuere velit aliquet. Morbi leo risus, porta ac consectetur ac, vestibulum at eros. Nullam quis risus eget urna mollis ornare vel eu leo.&lt;/p&gt;

&lt;h3 id=&quot;tables&quot;&gt;Tables&lt;/h3&gt;

&lt;p&gt;Aenean lacinia bibendum nulla sed consectetur. Lorem ipsum dolor sit amet, consectetur adipiscing elit.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Upvotes&lt;/th&gt;
      &lt;th&gt;Downvotes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tfoot&gt;
    &lt;tr&gt;
      &lt;td&gt;Totals&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bob&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Charlie&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Nullam id dolor id nibh ultricies vehicula ut id elit. Sed posuere consectetur est at lobortis. Nullam quis risus eget urna mollis ornare vel eu leo.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Want to see something else added? &lt;a href=&quot;https://github.com/poole/poole/issues/new&quot;&gt;Open an issue.&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 

</feed>
