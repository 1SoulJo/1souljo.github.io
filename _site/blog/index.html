<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog &middot; 1SoulJo
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png"> -->
  <link rel="shortcut icon" href="https://smilebasicsource.com/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
          <img src="/public/img/maschine.jpg"/>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>developer</strong></p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/1souljo">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:hansol.jo@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item active" href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    

    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2017 1SoulJo. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://codinfox.github.io">codinfox-lanyon</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home" title="1SoulJo">
              1SoulJo
            </a>
            <small>technical blog and study</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/study/2017/12/20/Ensemble-model/">
        Ensemble model
      </a>
    </h1>

    <span class="post-date">20 Dec 2017</span>
     | 
    
    <a href="/blog/tags/#ensemble" class="post-tag">ensemble</a>
    
    

    <article>
      <p>원본 : <a href="https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/">How to build Ensemble Models in machine learning?</a></p>

<p>지난 12개월 간 다양한 머신 러닝 해커톤에 참여해왔다. 시합이 끝나면 항상 1등의 솔루션을 확인해보는데, 보다보면 굉장한 insight 를 받기도 해서<br />
차후 시합에 큰 도움이 되곤 했다.<br />
승자들의 대부분은 잘 튜닝된 개별 모델의 앙상블(Ensemble) 과 더불어 feature engineering 을 사용하고 있었다.
이 두 방법을 잘 기억하기를 조언한다. 머신 러닝에 있어서 이 것들을 잘 사용하는 것이 아주 중요하기 때문이다.<br />
나는 대부분의 경우 feature engineering 은 잘 사용해왔지만, 앙상블은 사용하지 않았다. 만약 당신이 초보자라면 앙상블 모델에
최대한 빨리 익숙해지는 것이 좋을 것이다. (어쩌면 무의식 중에 이미 적용중이었을 수도 있다!)</p>

<p>이 포스트에서 앙상블 모델링의 기초에 대해 설명하고자 한다. 그리고 나서 앙상블의 장단점을 살펴보고, 실습을 제공할 것이다.</p>

<h2 id="1-ensemble-이란">1. Ensemble 이란?</h2>
<p>일반적으로 앙상블이란 2개 이상의 알고리즘 - base learner 라고 함 - 들을 결합해서 사용하는 기술을 의미한다.
모든 base learner 들의 예측 결과를 조합하여 사용하기 때문에 전체 시스템을 보다 탄탄하게 해준다.
여러명의 주식 거래자들이 한 방에 모여 주가가 오를지 내릴지를 토론하여 결정하는 것과 마찬가지라고 생각하면 되겠다.</p>

<p>각각의 거래자들이 모두 다른 정보와 이해를 가지고있기 때문에, 문제 상황으로부터 결론에 이르기까지의 공식은 다 다를 것이다.
따라서 주식 시장에 대한 각자의 이해를 바탕으로 다양한 주가 예측이 나오게 된다.</p>

<p>이제 최종 결론을 내릴 때가 되면 모든 다양한 예측들을 함께 고려해봐야할 것이다.
이로 인해 우리의 최종 결론은 더욱 탄탄(robust)하고, 정확하고, 덜 편중(biased)된 방향으로 결정될 수 있다.
최종 결론은 거래자 개인의 판단에 기반한 결론과 정 반대의 내용이 될 지도 모른다.</p>

<p>또 다른 예로 구직자의 인터뷰를 들 수 있다. 구직자가 가진 능력에 대한 최종 판단은 인터뷰 진행자들의 모든 피드백에 기반하여 결정된다.
인터뷰 진행자 한 명만으로는 요구 기술과 특성 각각에 대한 테스트를 모두 수행하지는 못할 수 있다.
하지만 여러명의 인터뷰 진행자들의 복합적인 피드백으로 인해 구직자에 대한 더 올바른 평가가 이루어 질 수 있는 것이다.</p>

<h2 id="2-types-of-ensembling">2. Types of ensembling</h2>
<p>더 세부적으로 진행하기 전에 꼭 알아야하는 기본 컨셉들이 있다:</p>
<ul>
  <li>
    <p><strong>Averaging</strong> :<br />
모델들의 예측 결과에 대한 평균을 취하는 것으로, regression 문제나 classification 확률 예측 문제에 모두 사용된다.
<img src="/public/img/Average.png" alt="Average" /></p>
  </li>
  <li>
    <p><strong>Majority vote</strong> :<br />
classification 문제를 해결할 때, 각각의 모델이 선정한 결과 중 최대 빈도로 선택된 결과를 취하는 방식이다.
<img src="/public/img/voting.png" alt="voting" /></p>
  </li>
  <li>
    <p><strong>Weighted average</strong> :<br />
Average 값들에 모델 별 가중치를 두는 방식이다.
<img src="/public/img/Wtaverage1.png" alt="Wtaverage" /></p>
  </li>
</ul>

<p>여러개의 모델들을 조합하는 방법은 이외에도 셀 수 없이 많을 것이다. 하지만 주로 사용되는 기술들이 있다:</p>
<ol>
  <li>
    <p><strong>Bagging</strong><br />
Bagging 은 <strong>B</strong>ootstrap <strong>agg</strong>regation 이라고도 불린다.
Bagging 을 이해하기 위해서는 먼저 bootstraping 을 이해할 필요가 있다. Bootstraping 은 n 개의 행을 가진 원본 dataset 으로부터 n 개의 행을 선택하는 샘플링 기법이다.
핵심은 각각의 행이 선택될 때 동일한 확률로 선택된다는 것이다.<br />
  1st sampling
  <img src="/public/img/BAGG1.png" alt="Wtaverage" /><br />
  2nd sampling
  <img src="/public/img/BAGG2.png" alt="Wtaverage" /><br />
  3rd sampling
  <img src="/public/img/BAGG3.png" alt="Wtaverage" /><br />
  4th sampling
  <img src="/public/img/BAGG4.png" alt="Wtaverage" /><br />
  랜덤하게 선택하기 때문에, 위와 같이 중복 선택이 허용된다.</p>

    <p>동일한 원본 data 로부터 여러개의 bootstrapped sample 들을 추출할 수 있는데, 추출된 sample 로부터 또 다시 bootstraping 을 하면
 tree 구조의 많은 sample set 을 구성할 수 있다. 그러한 sample set 을 기반으로 majority vote 나 averaging 기법을 사용해서
 최종 예측을 얻을 수 있다. Bagging 이란 이렇게 동작하는 것이다.</p>

    <p>여기서 눈여겨 보아야 할 것은, bagging 을 통해 분산(variance)를 줄일 수 있다는 점이다. Random forest 가 사실 이러한
 컨셉을 사용하는데, Random forest 는 한 발 더 나아가 각각의 bootstrap 된 sample 들의 feature subset 까지 랜덤하게 추출함을 통해
 분산을 줄인다. 이를 통해 학습 데이터를 구분(split) 할 수 있다.</p>
  </li>
  <li>
    <p><strong>Boosting</strong><br />
Boosting 은 여러개의 알고리즘을 순차적으로 적용하는 기법으로, 첫 번째 알고리즘이 전체 dataset 에 대한 학습을 진행하고,
다음 알고리즘들은 첫 번째 알고리즘이 실패한(남긴, residuals) 데이터에 맞추어 생성되는 방식이다.
따라서 다음 알고리즘들은 이전 모델이 잘 예측하지 못한 데이터에게 더 큰 가중치를 두게 된다.<br />
  이는 전체 dataset 에 대한 성능은 좋지 않지만, 특정 부분에 대한 성능은 좋은 learner 들의 집합으로 이루어진다.
  따라서 각각의 모델이 모여 앙상블 전체의 성능을 향상(boost) 시키는 것이다.<br />
  여기서 중요한 부분은, <i>boosting 은 편중(bias) 를 줄이는 데에 집중한다</i> 는 점이다.
  이로 인해 boosting 은 오버피팅을 유발하기 쉽다. 따라서 boosting 사용 시, 오버피팅을 피하기 위한 파라메터 튜닝이 굉장히 중요하다.<br />
  Boosting 의 대표적인 예제로는 XGBoost, GBM, ADABOOST 등이 있다.</p>
  </li>
  <li>
    <p><strong>Stacking</strong><br />
Stacking 은 여러개의 머신 러닝 모델들을 쌓아서 각각의 모델이 예측 결과를 상위 레이어의 모델에 전달하고,
최상위의 모델이 최종 결론을 도출하는 방식이다.
<img src="/public/img/stacking.png" alt="Stacking" />
위 그림에는 두 개의 머신 러닝 모델들이 존재하고 있다:</p>
    <ul>
      <li>하위 레이어의 모델들(d1, d2, d3) 는 원본 입력 feature들(x) 를 취한다.</li>
      <li>최상위 모델 f() 는 하위 레이어 모델의 출력을 받아 최종 출력을 예측한다.</li>
      <li>주목할 점은 예측된 결과 값들이 학습 data 로 사용된다는 점이다.</li>
    </ul>
  </li>
</ol>

<p>위에서는 2 개의 레이어만 사용되었지만, 다양한 수의 레이어가 사용될 수 있고, 레이어 내의 모델 역시 개수에 제한이 없다.
모델들을 선택함에 있어 두 가지 핵심 요건이 있는데:</p>
<ul>
  <li>개별 모델들은 정해둔 정확도 요건(criteria)을 충족시킬 것.</li>
  <li>개별 모델의 예측 값은 다른 모델의 예측과 큰 연관도(correlation)가 없어야 한다.</li>
</ul>

<p>최상위 모델은 하위 모델의 예측 결과를 입력값으로 받고 있는데, 이 최상위 모델 또한 아래와 같은 단순한 모델로 대체될 수 있다:</p>
<ul>
  <li>Averaging</li>
  <li>Majority vote</li>
  <li>Weighted average</li>
</ul>

<h2 id="3-ensemble-의-장단점">3. Ensemble 의 장단점</h2>
<h4 id="31-장점">3.1 장점</h4>
<ul>
  <li>앙상블은 모델의 정확도를 높이고, 대부분의 case 에 대해 잘 동작한다고 입증된 방식이다.</li>
  <li>거의 모든 머신 러닝 해커톤에 있어서, 1위를 하는데 필수적이다.</li>
  <li>앙상블은 모델을 더욱 탄탄하고 안정적이게 해주고, 따라서 대부분의 시나리오에서 괜찮은 성능을 보장해준다.</li>
  <li>Data 에 내재하는 선형적이고 단순한 관계 뿐 아니라 비선형적인 복잡한 관계까지 뽑아내는 데에 앙상블을 사용할 수 있다.<br />
2 개의 다른 모델을 사용해서 하나의 앙상블을 구성하면 된다.(?)</li>
</ul>

<h4 id="32-단점">3.2 단점</h4>
<ul>
  <li>앙상블은 모델의 해석력(interpretability)을 떨어뜨리고, 결국 비지니스 insight 를 이끌어내기가 굉장히 힘들어 질 수 있다.</li>
  <li>시간이 많이 걸리고, 실시간 어플리케이션 용으로는 적합하지 않을 수 있다.</li>
  <li>앙상블을 구성하는 모델을 잘 선택하는 방법을 익히는 것이 어렵다.</li>
</ul>

<h2 id="4-예제">4. 예제</h2>
<p><i>생략</i></p>

<h2 id="5-더-알아보기">5. 더 알아보기</h2>
<ul>
  <li><a href="https://www.analyticsvidhya.com/blog/2015/09/questions-ensemble-modeling/">5 Easy questions on Ensemble Modeling everyone should know</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2017/02/40-questions-to-ask-a-data-scientist-on-ensemble-modeling-techniques-skilltest-solution/">40 Questions to ask a Data Scientist on Ensemble Modeling Techniques (Skilltest Solution)</a></li>
</ul>


    <article>
    <div class="post-more">
      
      <a href="/study/2017/12/20/Ensemble-model/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/study/2017/11/30/conditional-random-filed/">
        CRF(Conditional Random Field)
      </a>
    </h1>

    <span class="post-date">30 Nov 2017</span>
     | 
    
    <a href="/blog/tags/#crf" class="post-tag">CRF</a>
    
    

    <article>
      <h2 id="crf-란">CRF 란?</h2>
<p>저스틴 비버의 하루 일상을 순서대로 찍은 사진들이 있다고 상상해보자.<br />
우리는 각각의 사진에 한 단어로 설명(라벨)을 달고자 한다. (예&gt; 식사 사진, 수면 사진, 운전 중 등등)<br />
어떻게 하면 될까?</p>

<p>한가지 방법은 사진들이 가지는 순서에 대한 정보는 무시하고 각 사진 자체만으로 분류를 해보는 것이 될 수 있겠다.
예를 들어 라벨이 달린 한 달 치의 사진이 주어졌다고 하면, 아침 6시 쯤에 찍힌 어두운 사진들은 ‘수면’ 과 연관이 있는 사진들이고,
밝은 색깔이 많이 포함된 사진은 ‘춤’ 에 대한 사진, 자동차가 나온 사진들은 ‘운전’ 사진이라는 등의 사실을 배울 수 있을 것이다.</p>

<p>하지만 순서에 대한 관점을 무시하면 많은 양의 정보를 잃게 된다. 예를 들어 입만 크게 나온 사진이 있다고 하자. 그건 노래하는 사진일까 밥 먹는 사진일까?
만약 이전의 사진이 저스틴 비버가 밥을 먹거나 요리하는 사진이었다면, 그 입 사진은 ‘먹는’ 행위에 관한 사진일 가능성이 더 높을 것이다.
하지만 이전 사진이 저스틴 비버가 노래나 춤 추는 사진이었다면, 입 사진도 마찬가지로 ‘노래하는’ 사진일 가능성이 높다.</p>

<h3 id="part-of-speech-tagging">Part-of-Speech Tagging</h3>
<p>part-of-speech tagging 를 예로 좀 더 자세히 살펴보도록 하자.<br />
POS tagging 의 목적은 한 문장의 어휘들의 품사를 달아주는 것이다. (ex&gt; 명사, 동사, 형용사, 명사)<br />
예를 들어 ‘Bob drank coffee at Starbucks.’ 라는 문장은 ‘Bob(명사) drank(동사) coffee(명사) at(전치사) Starbucks(명사)’ 가 될 것이다.
그럼 이제 CRF 를 이용해서 POS tagging 을 해보자. 먼저 다른 classifier 들과 마찬가지로 feature function <script type="math/tex">f_i</script> 들을 정의해야 한다.</p>

<h3 id="feature-functions-in-a-crf">Feature functions in a CRF</h3>
<p>CRF 에서 각각의 feature function 은 아래와 같은 입력들을 받는 함수이다:</p>
<ul>
  <li>s : 문장</li>
  <li>i : 단어의 문장 내 인덱스</li>
  <li><script type="math/tex">l_i</script> : 현재 단어의 라벨</li>
  <li><script type="math/tex">l_{i-1}</script> : 이전 단어의 라벨</li>
</ul>

<p>그리고 출력 값은 실수 값이 된다.(하지만 실제로는 0 또는 1 인 경우가 많다.)</p>

<p><em>참고 : 본 포스팅에서는 문장 전체에 대한 라벨이 아니라 현재 단어와 이전 단어의 라벨만 사용하도록 제한하고 있다.
이것은 <strong>linear-chain CRF</strong> 의 일종이라고 할 수 있다. 설명의 편의를 위해 범용적인 CRF 에 대한 내용은 생략하고자 한다.</em></p>

<h3 id="feature-를-probabilties-로">Feature 를 Probabilties 로.</h3>
<p>이제 각각의 feature function <script type="math/tex">f_j</script> 에 weight <script type="math/tex">\lambda_j</script> 를 할당해 보자.(데이터로부터 이 weight 들을 어떻게 학습하는 지는 아래에 설명하겠다.)<br />
문장 s 가 주어졌을 때, 문장 내 모든 단어에 대한 weighted feature 값을 더하여 s 에 라벨 l 이 부여될 수 있는지 점수를 매길 수 있다.</p>

<script type="math/tex; mode=display">score(l|s) = \sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})</script>

<p>(첫 번째 시그마는 각각의 feature function <script type="math/tex">f</script> 에 대한 것이고, 안쪽 시그마는 <script type="math/tex">i</script> 위치의 단어 각각에 대한 연산이다.)</p>

<p>그 다음 지수 연산(exponentiating)과 정규화(normalizing)를 통해 이 점수를 확률
<script type="math/tex">p(l | s)</script> 로 환산할 수 있다.</p>

<script type="math/tex; mode=display">p(l | s) = \frac{exp[score(l|s)]}{\sum_{l’} exp[score(l’|s)]} = \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})]}{\sum_{l’} exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l’_i, l’_{i-1})]}</script>

<h3 id="feature-function-예시">Feature Function 예시</h3>
<p>그럼 이 feature function 들은 어떻게 생겼을까? POS tagging 의 feature 들은 다음과 같을 수 있다:</p>
<ul>
  <li>
    <script type="math/tex; mode=display">% <![CDATA[
f_1(s, i, l_i, l_{i-1}) =
\begin{cases}
 1 & \mbox{if }l_i = \text{부사(ADVERB)} \mbox{ and the ith word ends in “-ly”} \\
 0 & \mbox{otherwise.}
 \end{cases} %]]></script>
    <ul>
      <li>이 feature 에 대한 weight <script type="math/tex">\lambda_1</script> 값이 양수이고 큰 경우, 이 feature 는 -ly 로 끝나는 단어를 ‘부사’ 로 라벨링할 거라는 의미가 될 것이다.</li>
    </ul>
  </li>
  <li>
    <script type="math/tex; mode=display">% <![CDATA[
f_2(s, i, l_i, l_{i-1}) =
\begin{cases}
1 & \mbox{if } i = 1, l_i = 동사(VERB), \mbox{ and the sentence ends in a question mark} \\
0 & \mbox{otherwise.}
\end{cases} %]]></script>
    <ul>
      <li>마찬가지로 feature 에 대한 weight <script type="math/tex">\lambda_2</script> 값이 양수이고 큰 경우, ? 로 끝나는 문장의 첫 단어를 ‘동사’ 로 라벨링하고자 할 것이다.</li>
    </ul>
  </li>
  <li>
    <script type="math/tex; mode=display">% <![CDATA[
f_3(s, i, l_i, l_{i-1}) =
\begin{cases}
1 & \mbox{if }l_{i-1} = \mbox{형용사(ADJECTIVE)} \mbox{, and }l_i = \mbox{명사(NOUN)} \\
0 & \mbox{otherwise.}
\end{cases} %]]></script>
    <ul>
      <li>weight 가 양수 인 경우, 이 feature 는 형용사 뒤에는 명사가 온다는 것을 의미한다.</li>
    </ul>
  </li>
  <li>
    <script type="math/tex; mode=display">f_4(s, i, l_i, l_{i-1}) = 1 \mbox{ if }l_{i-1} =\mbox{ 전치사(PREPOSITION) and }l_i = \mbox{전치사(PREPOSITION).}</script>
    <ul>
      <li>이 함수에 대한 음수의 weight <script type="math/tex">\lambda_4</script> 는 전치사 뒤에는 전치시가 오면 안되다는 것을 의미한다. 따라서 그런 라벨링이 발생하지 않도록 할 수 있을 것이다.</li>
    </ul>
  </li>
</ul>

<p>이게 끝이다!<br />
정리해보면 CRF 를 만들기 위해서는,</p>
<ol>
  <li>feature function 몇 개를 정의해주고, (feature function 은 전체 문장, 현재 위치, 그리고 주변의 라벨 값을 참조한다.)</li>
  <li>weight 를 할당하고</li>
  <li>전체를 몽땅 더한 다음,</li>
  <li>필요하면 그 값을 확률로 환산하면 된다.</li>
</ol>

<p>이제 CRF 이 다른 머신 러닝 기술들와 어떻게 다른지 비교해보도록 하자.</p>

<h2 id="smells-like-logistic-regression">Smells like Logistic Regression…</h2>
<p>CRF 의 아래 식은 왠지 친숙해 보일 수도 있다. <em><a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank">궁금하면 여기</a></em></p>

<script type="math/tex; mode=display">p(l | s) = \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n f_j(s, i, l_i, l_{i-1})]}{\sum_{l’} exp[\sum_{j = 1}^m \sum_{i = 1}^n f_j(s, i, l’_i, l’_{i-1})]}</script>

<p>왜냐하면 CRF 는 근본적으로 Logistic Regression 의 순차 버전(sequential version)이기 때문이다.</p>
<blockquote>

  <p>Logistic Regression 이 분류(claasification) 을 위한 log-linear 모델인데 반해,<br />
CRF 는 순차적인 라벨링(sequential labels) 을 위한 log-linear 모델이다.</p>
</blockquote>

<h2 id="looks-like-hmms">Looks like HMMs…</h2>
<p>HMM(<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model" target="_blank">Hidden Markov Model</a>) 도 POS tagging(또한 일반적인 순차 라벨링) 에 사용될 수 있는 모델임을 기억하자.<br />
CRF 가 라벨링 점수를 계산하기 위해 여러 개의 feature function 을 다 같이 사용하는데 비해,<br />
HMM 은 라벨링을 위해 생성적(generative) 방식을 취한다. 그 식은 아래와 같다:</p>

<script type="math/tex; mode=display">p(l,s) = p(l_1) \prod_i p(l_i | l_{i-1}) p(w_i | l_i)</script>

<p>이 때,</p>
<ul>
  <li>$
p(l_i | l_{i-1})$ 는 전이 확률(transition probabilities) - 예&gt; 전치사 뒤에 명사가 나올 확률</li>
  <li>$
p(w_i | l_i)$ 는 출력 확률(emission probabilities) - 예&gt; ‘명사’ 라벨이 ‘아빠’ 를 도출할 확률</li>
</ul>

<p>그럼 HMM 과 CRF 는 어떻게 다른가?<br />
<strong>CRF 가 더 강력하다!</strong><br />
CRF 는 HMM 이 할 수 있는 모든 것을 모델링할 수 있고, 그것보다 더 많은 일을 해낸다. 왜 그런지는 아래를 살펴보자.</p>

<p>HMM 확률 식에 log 를 취하면 아래와 같다.</p>

<script type="math/tex; mode=display">\log p(l,s) = \log p(l_0) + \sum_i \log p(l_i | l_{i-1}) + \sum_i \log p(w_i | l_i)</script>

<p>이 log-확률을 2진(binary) 전이, 출력 feature 들에 할당된 weight 라고 간주하면, 위 식은 CRF 의 log-linear 형태와 완전히 일치한다.
다시 말해, 어떠한 HMM 이라도 그와 동일한 CRF 를 구현할 수 있다는 뜻이다. 그 방법은 아래와 같다.</p>

<ul>
  <li>HMM 의 전이 확률 $p(l_i = y | l_{i-1} = x)$ 각각에 대해 CRF 전이 feature 를 아래처럼 정의한다.<br />
$f_{x,y}(s, i, l_i, l_{i-1}) = 1 \text{ if }l_i = y \text{, and }l_{i-1} = x$<br />
그리고 각 feature 에 다음과 같은 weight 를 부여한다.<br />
$w_{x,y} = \log p(l_i = y | l_{i-1} = x)$</li>
  <li>위와 유사하게, HMM 의 출력 확률 $p(w_i = z | l_{i} = x)$ 각각에 대해 CRF 출력 feature 를 아래처럼 정의한다.<br />
$g_{x,y}(s, i, l_i, l_{i-1}) = 1 \text{ if }w_i = z\text{, and }l_i = x$<br />
그리고 각 feature 에 다음과 같은 weight 를 부여한다.<br />
$w_{x,z} = \log p(w_i = z | l_i = x)$</li>
</ul>

<p>그러면 이 feature function 들을 사용하여 계산된 <script type="math/tex">p(l|s)</script> 점수는 해당하는 HMM 에 의해 계산된 점수에 정확히 비례하게 되고, 따라서 모든 HMM 은 특정 CRF 로 표현이 가능해진다.</p>

<p>하지만 CRF 훨씬 풍부한 라벨 분포 set 를 모델링할 수 있는데, 이 이유는 다음과 같다.</p>
<ul>
  <li><strong>CRF 는 훨씬 많은 수의 feature set 를 정의할 수 있다.</strong> HMM 은 태생적으로 지엽적(local) 속성을 가진다.
왜냐면 HMM 은 2진 전이, 출력 feature function 만 사용 가능하고, 이로 인해 각 단어는 오직 현재의 라벨에, 그리고 각 라벨은 바로 이전의 라벨에만 의존하게 되기 때문이다.
이에 반해 CRF 는 좀더 전역적(global) feature 들을 사용할 수 있다. 예를 들어 위에서 보여준 POS tagger 의 feature 들 중 하나는 <em>문장의 끝이 ? 인 경우
문장의 제일 첫 단어가 동사</em> 로 태깅될 확률을 높이고 있다.</li>
  <li><strong>CRF 는 임의(arbitrary)의 weight 값들을 가질 수 있다.</strong> HMM 의 확률 값들은 어떤 조건 - 예를 들면,  $
0 &lt;= p(w_i | l_i) &lt;= 1, \sum_w p(w_i = w | l_1) = 1$ - 을 반드시 충족시켜야만 한다. 그에 비해 CRF 의 weight 는 아무 제약이 없다.<br />
$\log p(w_i | l_i)$ 는 어떠한 값이든 될 수 있다.</li>
</ul>

<h3 id="weight-의-학습">Weight 의 학습</h3>
<p>CRF 에서 weight 를 어떻게 학습시키는지에 대한 질문으로 다시 되돌아가자. <strong>gradient ascent (짜잔)</strong> 를 사용하는 것이 하나의 방법이 될 수 있다.</p>

<p>학습 데이터(문장들과 그에 대한 POS 라벨들)가 충분히 있다고 가정해보자. 먼저 CRF 모델의 weight 들을 임의의 값으로 초기화 한다.
이 무작위 weight 값들을 정답으로 바꿔나가려면 각각의 학습 데이터에 다음과 같은 작업을 해주면 된다.</p>
<ul>
  <li>각각의 feature function <script type="math/tex">f_i</script> 에서 학습 데이터의 log 확률에 대한 gradient 를 계산한다.
<script type="math/tex">\lambda_i \text{ : }\frac{\partial}{\partial w_j} \log p(l | s) = \sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l’} p(l’ | s) \sum_{j = 1}^m f_i(s, j, l’_j, l’_{j-1})</script></li>
  <li>gradient 의 첫 번째 term 은 <em>true</em> 라벨을 갖는 feature <script type="math/tex">f_i</script> 의 분포를,
 두 번째 term 은 현재 모델에서 feature <script type="math/tex">f_i</script> 의 <em>expected</em> 값의 분포를 나타냄을 주목하자.</li>
  <li><script type="math/tex">\lambda_i</script> 를 gradient 방향으로 이동시킨다 :<br />
<script type="math/tex">\lambda_i = \lambda_i + \alpha [\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l’} p(l’ | s) \sum_{j = 1}^m f_i(s, j, l’_j, l’_{j-1})]</script> where $\alpha$ is some learning rate.</li>
  <li>특정 조건(더 이상 갱신이 안된다던가)에 도달할 때까지 위의 작업을 반복한다.</li>
</ul>

<h3 id="최적-라벨-찾기">최적 라벨 찾기</h3>
<p>우리의 CRF 모델을 학습시켰고 이제 새로운 문장이 입력된다고 생각해보자. 라벨링은 어떻게 되나?</p>

<p>단순한 방법은 모든 가능한 라벨 l 에 대한 <script type="math/tex">p(l | s)</script> 를 계산하고, 이 확률을 최대화 시키는 라벨을 고르는 것이 될테다.
하지만 m 길이의 문장과 k 개의 tag 셋이 있다면 총 <script type="math/tex">k^m</script> 개의 조합이 가능하기 때문에, 이 방식은 지수승의 연산을 필요로 한다.</p>

<p>더 좋은 방법이 있다. (linear-chain) CRF 는 <a href="https://en.wikipedia.org/wiki/Optimal_substructure" target="_blank">optimal substructure</a> 속성을 만족하기 때문에, (polynomial-time 복잡도의) 동적 프로그래밍 알고리즘으로 최적 라벨을 찾을 수 있다. 이는 HMM 에 <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank">Viterbi 알고리즘</a>을 적용하는 것과 유사하다.</p>

<h3 id="좀-더-재밌는-응용">좀 더 재밌는 응용</h3>
<p>사실 part-of-speech tagging 은 좀 지루하고, 다른 POS 태거도 이미 많다. 실 생활에서 CRF 는 어디다가 써먹을 수 있을까?</p>

<p>트위터에서 사람들이 크리스마스에 받은 선물의 타입을 마이닝하고 싶다. 고 해보자.</p>

<p><img src="/public/img/CRF_Twitter.jpg" alt="twitter" /></p>

<p>그러면 문장에서 어느 단어가 선물에 해당하는지 어떻게 찾아낼 수 있나??!!1</p>

<p>위와 같은 그래프를 만들기 위해 나는 그냥 “크리스마스에 XXX 가 갖고싶어요.” 와 “크리스마스에 XXX 를 받았어요!” 형태의 문장을 검색했을 뿐이다.
하지만 더 세련된 CRF 버전으로, part-of-speech 에서의 품사 같이 GIFT 라는 태그 (한 발 더 나가면 GIFT-GIVER, GIFT-RECEIVER 를 이용해
  선물을 준사람과 받은사람의 정보까지) 를 사용해서 이것을 POS 태깅 문제처럼 해결해 볼 수 있을 것이다. 그 때 feature 는,</p>
<ul>
  <li>“이전 단어가 GIFT-RECEIVER 이고  그 이전 단어가 ‘gave’ 라면, 현재 단어는 GIFT 임”</li>
  <li>“다음 두 단어가 ‘for Christmas’ 면 현재 단어는 GIFT 임”</li>
</ul>

<p>와 같은 형태가 될 수 있을 것이다.</p>

<h3 id="마무리">마무리</h3>
<p>좀더 ‘랜덤한’ 생각을 나열하고 끝내고자 한다.</p>
<ul>
  <li>이 포스트에서는 CRF 를 사용한 그래픽 모델 프레임웍에 대한 내용은 일부러 스킵했다. 왜냐면 CRF 의 기본을 이해하는데는 큰 도움이 되지 않기 때문이다.
하지만 더 알고싶다면 Daphne Koller 가 공짜로 가르쳐주는 <a href="http://www.pgm-class.org" target="_blank">온라인 강좌</a>를 들어보시길.</li>
  <li>아니면 CRF 를 적용한 많은 NLP 응용 분야(POS 태깅이나 <a href="http://en.wikipedia.org/wiki/Named-entity_recognition" target="_blank">named entity extraction</a>등)에 관심이 있다면
Manning 과 Jurafsky 가 강의를 하고 있다. (링크가 있었는데 깨짐. http://www.nlp-class.org)</li>
  <li>또한 CRF/HMM 과 Logistic Regression/Naive Bayes 간의 유사점에 대해 조금 더 꾸며봤다. 아래 이미지를 참고바람.
<img src="/public/img/graph_model.png" alt="graph_model" /></li>
</ul>

<p>출처 : <a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields" target="_blank">introduction to conditional random fields</a></p>


    <article>
    <div class="post-more">
      
      <a href="/study/2017/11/30/conditional-random-filed/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/study/jsmath/tex/2017/11/29/js-math/">
        jsMath(TeX) 주요 문법
      </a>
    </h1>

    <span class="post-date">29 Nov 2017</span>
     | 
    
    <a href="/blog/tags/#jsmath" class="post-tag">jsMath</a>
    
    <a href="/blog/tags/#tex" class="post-tag">TeX</a>
    
    

    <article>
      <h2 id="jsmathtex-in-markdown">jsMath(TeX) in markdown</h2>

<p>마크다운 문서에서는 아래와 같이 $$ 부호 내에 입력하면 자동 전환됨.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$$
y = x^2
$$
</code></pre></div></div>

<p>결과 :<br />
<script type="math/tex">y = x^2</script></p>

<h3 id="1-산술-함수">1. 산술 함수</h3>

<table>
  <thead>
    <tr>
      <th>TeX</th>
      <th style="text-align: center">결과</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\exp_a b = a^b</td>
      <td style="text-align: center"><script type="math/tex">\exp_a b = a^b</script></td>
    </tr>
    <tr>
      <td>\exp b = e^b</td>
      <td style="text-align: center"><script type="math/tex">\exp b = e^b</script></td>
    </tr>
    <tr>
      <td>10^m, \ln c, \lg d = \log e</td>
      <td style="text-align: center"><script type="math/tex">10^m, \ln c, \lg d = \log e</script></td>
    </tr>
    <tr>
      <td>\log_{10} f</td>
      <td style="text-align: center"><script type="math/tex">\log_{10} f</script></td>
    </tr>
    <tr>
      <td>\min(x,y), \max(x,y)</td>
      <td style="text-align: center"><script type="math/tex">\min(x,y), \max(x,y)</script></td>
    </tr>
  </tbody>
</table>

<h3 id="2-미분">2. 미분</h3>

<table>
  <thead>
    <tr>
      <th>TeX</th>
      <th style="text-align: center">결과</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>dt, \partial t, {dy \over dx}</td>
      <td style="text-align: center"><script type="math/tex">dt, \partial t, {dy \over dx}</script></td>
    </tr>
    <tr>
      <td>{\partial^2\over\partial x_1\partial x_2}y</td>
      <td style="text-align: center"><script type="math/tex">{\partial^2\over\partial x_1\partial x_2}y</script></td>
    </tr>
    <tr>
      <td>f’, f’’, f^{(3)}</td>
      <td style="text-align: center"><script type="math/tex">f', f'', f^{(3)}</script></td>
    </tr>
    <tr>
      <td>\dot y</td>
      <td style="text-align: center"><script type="math/tex">\dot y</script></td>
    </tr>
  </tbody>
</table>

<h3 id="3-근호">3. 근호</h3>

<table>
  <thead>
    <tr>
      <th>TeX</th>
      <th style="text-align: center">결과</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\sqrt{2}, \sqrt[n]{x^5}</td>
      <td style="text-align: center"><script type="math/tex">\sqrt{2}, \sqrt[n]{x^5}</script></td>
    </tr>
    <tr>
      <td>\sqrt[3]{x^3+y^3 \over 2}</td>
      <td style="text-align: center"><script type="math/tex">\sqrt[3]{x^3+y^3 \over 2}</script></td>
    </tr>
  </tbody>
</table>

<h3 id="4-순열과-조합">4. 순열과 조합</h3>

<table>
  <thead>
    <tr>
      <th>TeX</th>
      <th style="text-align: center">결과</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>{}<em>nP</em>{k}, <em>{n}C</em>{k}</td>
      <td style="text-align: center"><script type="math/tex">{}_nP_{k}, \; _{n}C_{k}</script></td>
    </tr>
    <tr>
      <td>^{n}P_{k}, P_{k}^{n}</td>
      <td style="text-align: center"><script type="math/tex">^{n}P_{k}, \; P_{k}^{n}</script></td>
    </tr>
    <tr>
      <td>C(n,k)</td>
      <td style="text-align: center"><script type="math/tex">C(n,k)</script></td>
    </tr>
  </tbody>
</table>

<h3 id="5-집합">5. 집합</h3>

<table>
  <thead>
    <tr>
      <th>TeX</th>
      <th style="text-align: center">결과</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>{ }</td>
      <td style="text-align: center"><script type="math/tex">\{ \}</script></td>
    </tr>
    <tr>
      <td>\in, \notin, \ni, \not\ni</td>
      <td style="text-align: center"><script type="math/tex">\in, \notin, \ni, \not\ni</script></td>
    </tr>
    <tr>
      <td>\cap, \cup, \subset, \supset</td>
      <td style="text-align: center"><script type="math/tex">\cap, \cup, \subset, \supset</script></td>
    </tr>
  </tbody>
</table>

<h3 id="6-위-아래-전치-후치-첨자">6. 위, 아래, 전치, 후치, 첨자</h3>

<table>
  <thead>
    <tr>
      <th>TeX</th>
      <th style="text-align: center">결과</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>y_m,   a_{i,j},   x_2^3</td>
      <td style="text-align: center"><script type="math/tex">y_m, \; a_{i,j}, \; x_2^3</script></td>
    </tr>
    <tr>
      <td>{}<em>{b}^{a}X,   _{c}^{a}C</em>{d}^{b}</td>
      <td style="text-align: center"><script type="math/tex">{}_{b}^{a}X, \; _{c}^{a}C_{d}^{b}</script></td>
    </tr>
    <tr>
      <td>\overset{x}{P},   \underset{y}{Z}</td>
      <td style="text-align: center"><script type="math/tex">\overset{x}{P}, \; \underset{y}{Z}</script></td>
    </tr>
  </tbody>
</table>

<h3 id="7-기타">7. 기타</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th>TeX</th>
      <th style="text-align: center">결과</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">공백</td>
      <td>a \quad b, a\ b</td>
      <td style="text-align: center"><script type="math/tex">a \quad b, a\ b</script></td>
    </tr>
    <tr>
      <td style="text-align: center">색깔</td>
      <td>{\color{Blue}x^2}+{\color{Red}2x}-{\color{Green}1}</td>
      <td style="text-align: center"><script type="math/tex">{\color{Blue}x^2}+{\color{Red}2x}-{\color{Green}1}</script></td>
    </tr>
    <tr>
      <td style="text-align: center">한글</td>
      <td>\text{한 글}</td>
      <td style="text-align: center"><script type="math/tex">\text{한 글}</script></td>
    </tr>
    <tr>
      <td style="text-align: center">괄호</td>
      <td>\left( \frac{1}{2} \right)</td>
      <td style="text-align: center"><script type="math/tex">\left( \frac{1}{2} \right)</script></td>
    </tr>
    <tr>
      <td style="text-align: center">집합 괄호</td>
      <td>\left\{ A \right\}</td>
      <td style="text-align: center"><script type="math/tex">\left\{ A \right\}</script></td>
    </tr>
    <tr>
      <td style="text-align: center">시그마</td>
      <td>\sum_{k=1}^N k^2</td>
      <td style="text-align: center"><script type="math/tex">\sum_{k=1}^N k^2</script></td>
    </tr>
    <tr>
      <td style="text-align: center">곱기호</td>
      <td>\prod_{i=1}^N x_i</td>
      <td style="text-align: center"><script type="math/tex">\prod_{i=1}^N x_i</script></td>
    </tr>
    <tr>
      <td style="text-align: center">극한</td>
      <td>\lim_{n \to \infty}x_n</td>
      <td style="text-align: center"><script type="math/tex">\lim_{n \to \infty}x_n</script></td>
    </tr>
    <tr>
      <td style="text-align: center">적분</td>
      <td>\int_{-N}^{N} e^x\, dx</td>
      <td style="text-align: center"><script type="math/tex">\int_{-N}^{N} e^x\, dx</script></td>
    </tr>
  </tbody>
</table>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>f(n)=
\begin{cases}
n/2, &amp; \mbox{if }n\mbox{ is even} \\
3n+1, &amp; \mbox{if }n\mbox{ is odd}
\end{cases}
</code></pre></div></div>
<p><script type="math/tex">% <![CDATA[
f(n)=
\begin{cases}
n/2, & \mbox{if }n\mbox{ is even} \\
3n+1, & \mbox{if }n\mbox{ is odd}
\end{cases} %]]></script></p>

    <article>
    <div class="post-more">
      
      <a href="/study/jsmath/tex/2017/11/29/js-math/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/study/2017/11/29/gibbs-sampling/">
        Gibbs Sampling
      </a>
    </h1>

    <span class="post-date">29 Nov 2017</span>
     | 
    
    <a href="/blog/tags/#gibbs-sampling" class="post-tag">gibbs-sampling</a>
    
    

    <article>
      <h2 id="정의">정의</h2>
<p>In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables). Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.</p>

<p>Gibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference. It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM).</p>

<p>As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired. Generally, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution and are usually discarded. If necessary, one possible remedy is thinning the resulting chain of samples (i.e. only taking every nth value, e.g. every 10th value). It has been shown, however, that using a longer chain instead (e.g. a chain that is n times as long as the initially considered chain using a thinning factor of n) leads to better estimates of the true posterior. Thus, thinning should only be applied when time or computer memory are restricted.[1]</p>

<p>hello
<script type="math/tex">y = \frac{x ^ 2}{2} K\left(1-\sqrt{x^3 - 8}\right)</script>
world</p>

<p>출처 : <a href="https://en.wikipedia.org/wiki/Gibbs_sampling#Introduction" target="_blank">Gibbs Sampling</a></p>

    <article>
    <div class="post-more">
      
      <a href="/study/2017/11/29/gibbs-sampling/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/study/2017/11/29/Monte-Carlo-Method/">
        Monte Carlo Method
      </a>
    </h1>

    <span class="post-date">29 Nov 2017</span>
     | 
    
    <a href="/blog/tags/#monte-carlo-method" class="post-tag">monte-carlo-method</a>
    
    

    <article>
      <h2 id="정의">정의</h2>
<p>난수를 이용하여 함수의 값을 확률적으로 계산하는 알고리즘.</p>
<h4 id="핵심-아이디어">핵심 아이디어</h4>
<blockquote>
  <p>이론상 determistic 할지도 모르는 문제에 대해 randomness 를 사용하여 해결한다!</p>
</blockquote>

<p>주로 물리/수학적 문제들에 대해 다른 방법으로 해결할 수 없는 경우에 사용한다.(ㅋㅋ)</p>

<h4 id="주요-적용-분야">주요 적용 분야</h4>
<ol>
  <li>최적화 문제</li>
  <li>numerical integration (수치적 미분?)</li>
  <li>확률 분포로부터 draw 를 생성 (generating draw from probability distribution)</li>
</ol>

<h4 id="어떻게-동작하나">어떻게 동작하나?</h4>
<p>일반적으로 아래 패턴으로 동작함.</p>
<ol>
  <li>입력 가능한 데이터의 도메인을 정의한다.</li>
  <li>해당 도메인에 대한 확률 분포로부터 랜덤 데이터를 생성</li>
  <li>입력 데이터에 대한 deterministic 계산을 수행</li>
  <li>결과 수집</li>
</ol>

<p>예를 들어, (x,y) 그래프의 1 사분면에 (0, 0) 을 중점으로 하고 반지름이 1 인 원이 있다고 생각해보자. 원의 넓이는 π/4 일 것이다.<br />
이 때 Monte Carlo Method 를 이용해 π 값을 근사해 보면,</p>
<ol>
  <li>한 변의 길이가 1인 정사각형을 그리고, 그 안에 반지름이 1 인 원을 그린다. (1/4 원)</li>
  <li>균등한 크기의 물체를 사각형 안에 균등하게 흩뿌린다.(uniformly scatter)</li>
  <li>원 안에 들어간 물체의 갯수와 전체 갯수를 센다.</li>
  <li>원 안에 들어간 물체의 갯수와 전체 갯수의 비율은 두 영역의 비율과 같으므로, 즉 π/4 가 된다.<br />
해당 비율에 4을 곱해서 π 의 근사치를 구할 수 있다.</li>
</ol>

<p>여기서 중요한 포인트 2가지.</p>
<blockquote>

  <ol>
    <li>흩뿌릴 때 균등하게 분포되지 않으면 결과가 좋지 않을 것이다.</li>
    <li>입력 데이터의 갯수가 많아야 한다. 많을 수록 결과가 개선될 것이다.</li>
  </ol>
</blockquote>

<p><img src="/public/img/monte.gif" alt="monte carlo method" /></p>

<p>Monte Carlo Method 를 적용하기 위해서는 많은 수의 랜덤 값들이 요구되는데, 이로 인하여 pseudorandom number generator 의 개발에 박차를 가하게 되었다. pseudorandom number generator 는 기존에 통계적 샘플링에서 주로 사용되던 랜덤 숫자 테이블(table of random numbers) 대비 훨씬훨씬 성능이 좋았다.</p>

<p>출처 : <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method" target="_blank">Monte Carlo method</a></p>

    <article>
    <div class="post-more">
      
      <a href="/study/2017/11/29/Monte-Carlo-Method/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>
    
    <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
    </script>
    
  </body>
  
</html>
